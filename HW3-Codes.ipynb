{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "#import the packages\r\n",
    "import nltk\r\n",
    "import re\r\n",
    "\r\n",
    "# importing the guttenburg package\r\n",
    "from gutenberg.acquire import load_etext\r\n",
    "from gutenberg.cleanup import strip_headers\r\n",
    "\r\n",
    "#call Anna K from Gutenberg Library\r\n",
    "test_set = strip_headers(load_etext(1399)).strip()\r\n",
    "\r\n",
    "#Split the Sentences into Tokens\r\n",
    "TextToken = nltk.sent_tokenize(test_set)\r\n",
    "TextToken = TextToken[2:] # Removes unrelated text at the front about the Gutenberg Project\r\n",
    "print(TextToken[:5])\r\n",
    "SentTokens = [nltk.word_tokenize(sent) for sent in TextToken] #Tokenize each word in the sentence\r\n",
    "print(SentTokens[:5])\r\n",
    "test_set = SentTokens\r\n",
    "\r\n",
    "# To Save Memory, removing the additional data sets\r\n",
    "del TextToken\r\n",
    "del SentTokens"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The wife had\\r\\ndiscovered that the husband was carrying on an intrigue with a French\\r\\ngirl, who had been a governess in their family, and she had announced\\r\\nto her husband that she could not go on living in the same house with\\r\\nhim.', 'This position of affairs had now lasted three days, and not only\\r\\nthe husband and wife themselves, but all the members of their family\\r\\nand household, were painfully conscious of it.', 'Every person in the\\r\\nhouse felt that there was no sense in their living together, and that\\r\\nthe stray people brought together by chance in any inn had more in\\r\\ncommon with one another than they, the members of the family and\\r\\nhousehold of the Oblonskys.', 'The wife did not leave her own room, the\\r\\nhusband had not been at home for three days.', 'The children ran wild all\\r\\nover the house; the English governess quarreled with the housekeeper,\\r\\nand wrote to a friend asking her to look out for a new situation for\\r\\nher; the man-cook had walked off the day before just at dinner time;\\r\\nthe kitchen-maid, and the coachman had given warning.']\n",
      "[['The', 'wife', 'had', 'discovered', 'that', 'the', 'husband', 'was', 'carrying', 'on', 'an', 'intrigue', 'with', 'a', 'French', 'girl', ',', 'who', 'had', 'been', 'a', 'governess', 'in', 'their', 'family', ',', 'and', 'she', 'had', 'announced', 'to', 'her', 'husband', 'that', 'she', 'could', 'not', 'go', 'on', 'living', 'in', 'the', 'same', 'house', 'with', 'him', '.'], ['This', 'position', 'of', 'affairs', 'had', 'now', 'lasted', 'three', 'days', ',', 'and', 'not', 'only', 'the', 'husband', 'and', 'wife', 'themselves', ',', 'but', 'all', 'the', 'members', 'of', 'their', 'family', 'and', 'household', ',', 'were', 'painfully', 'conscious', 'of', 'it', '.'], ['Every', 'person', 'in', 'the', 'house', 'felt', 'that', 'there', 'was', 'no', 'sense', 'in', 'their', 'living', 'together', ',', 'and', 'that', 'the', 'stray', 'people', 'brought', 'together', 'by', 'chance', 'in', 'any', 'inn', 'had', 'more', 'in', 'common', 'with', 'one', 'another', 'than', 'they', ',', 'the', 'members', 'of', 'the', 'family', 'and', 'household', 'of', 'the', 'Oblonskys', '.'], ['The', 'wife', 'did', 'not', 'leave', 'her', 'own', 'room', ',', 'the', 'husband', 'had', 'not', 'been', 'at', 'home', 'for', 'three', 'days', '.'], ['The', 'children', 'ran', 'wild', 'all', 'over', 'the', 'house', ';', 'the', 'English', 'governess', 'quarreled', 'with', 'the', 'housekeeper', ',', 'and', 'wrote', 'to', 'a', 'friend', 'asking', 'her', 'to', 'look', 'out', 'for', 'a', 'new', 'situation', 'for', 'her', ';', 'the', 'man-cook', 'had', 'walked', 'off', 'the', 'day', 'before', 'just', 'at', 'dinner', 'time', ';', 'the', 'kitchen-maid', ',', 'and', 'the', 'coachman', 'had', 'given', 'warning', '.']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "#Read in the first Corpus\r\n",
    "\r\n",
    "File = open('J:\\My Drive\\Graduate School\\IST664 Natural Language Processing\\Homework Corpus\\Anna Karenina by Leo Tolestoy.txt', encoding = 'utf8')\r\n",
    "test_set = File.read()\r\n",
    "File.close()\r\n",
    "test_set[:150]\r\n",
    "\r\n",
    "#Split the Sentences into Tokens\r\n",
    "TextToken = nltk.sent_tokenize(test_set)\r\n",
    "TextToken = TextToken[2:] # Removes unrelated text at the front about the Gutenberg Project\r\n",
    "print(TextToken[:5])\r\n",
    "SentTokens = [nltk.word_tokenize(sent) for sent in TextToken] #Tokenize each word in the sentence\r\n",
    "print(SentTokens[:5])\r\n",
    "test_set = SentTokens\r\n",
    "\r\n",
    "# To Save Memory, removing the additional data sets\r\n",
    "del TextToken\r\n",
    "del SentTokens"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[Illustration]\\n\\n\\n\\n\\n ANNA KARENINA \\n\\n by Leo Tolstoy \\n\\n Translated by Constance Garnett \\n\\nContents\\n\\n\\n PART ONE\\n PART TWO\\n PART THREE\\n PART FOUR\\n PART FIVE\\n PART SIX\\n PART SEVEN\\n PART EIGHT\\n\\n\\n\\n\\nPART ONE\\n\\nChapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its\\nown way.', 'Everything was in confusion in the Oblonskys’ house.', 'The wife had\\ndiscovered that the husband was carrying on an intrigue with a French\\ngirl, who had been a governess in their family, and she had announced\\nto her husband that she could not go on living in the same house with\\nhim.', 'This position of affairs had now lasted three days, and not only\\nthe husband and wife themselves, but all the members of their family\\nand household, were painfully conscious of it.', 'Every person in the\\nhouse felt that there was no sense in their living together, and that\\nthe stray people brought together by chance in any inn had more in\\ncommon with one another than they, the members of the family and\\nhousehold of the Oblonskys.']\n",
      "[['[', 'Illustration', ']', 'ANNA', 'KARENINA', 'by', 'Leo', 'Tolstoy', 'Translated', 'by', 'Constance', 'Garnett', 'Contents', 'PART', 'ONE', 'PART', 'TWO', 'PART', 'THREE', 'PART', 'FOUR', 'PART', 'FIVE', 'PART', 'SIX', 'PART', 'SEVEN', 'PART', 'EIGHT', 'PART', 'ONE', 'Chapter', '1', 'Happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.'], ['Everything', 'was', 'in', 'confusion', 'in', 'the', 'Oblonskys', '’', 'house', '.'], ['The', 'wife', 'had', 'discovered', 'that', 'the', 'husband', 'was', 'carrying', 'on', 'an', 'intrigue', 'with', 'a', 'French', 'girl', ',', 'who', 'had', 'been', 'a', 'governess', 'in', 'their', 'family', ',', 'and', 'she', 'had', 'announced', 'to', 'her', 'husband', 'that', 'she', 'could', 'not', 'go', 'on', 'living', 'in', 'the', 'same', 'house', 'with', 'him', '.'], ['This', 'position', 'of', 'affairs', 'had', 'now', 'lasted', 'three', 'days', ',', 'and', 'not', 'only', 'the', 'husband', 'and', 'wife', 'themselves', ',', 'but', 'all', 'the', 'members', 'of', 'their', 'family', 'and', 'household', ',', 'were', 'painfully', 'conscious', 'of', 'it', '.'], ['Every', 'person', 'in', 'the', 'house', 'felt', 'that', 'there', 'was', 'no', 'sense', 'in', 'their', 'living', 'together', ',', 'and', 'that', 'the', 'stray', 'people', 'brought', 'together', 'by', 'chance', 'in', 'any', 'inn', 'had', 'more', 'in', 'common', 'with', 'one', 'another', 'than', 'they', ',', 'the', 'members', 'of', 'the', 'family', 'and', 'household', 'of', 'the', 'Oblonskys', '.']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "#using the Airline Services dataset for training on sentiment polarity\r\n",
    "import pandas as pd\r\n",
    "train_dataset = pd.read_csv('train_tweets_airlines.csv')\r\n",
    "#displaying the dataset using pandas\r\n",
    "display(len(train_dataset))\r\n",
    "display(train_dataset.head(2))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                        0.0  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold      name negativereason_gold  retweet_count  \\\n",
       "0                    NaN   cairdin                 NaN              0   \n",
       "1                    NaN  jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# Since this is social media data, we will have to add a few extra preprocessing steps\r\n",
    "# First, let's remove  @ and # (Twitter platform affordances) from the training data\r\n",
    "# We'll use regular expressions for that, creating a Function that we can use to pass the data through\r\n",
    "# NOTE: you could add more regexes to clean noisy characters such as emoticons, numbers, etc\r\n",
    "def remove_at(x):\r\n",
    "    x = str(x).replace('@', '')\r\n",
    "    x = str(x).replace('#', '')\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "#applying the clearning function and visualizing the result\r\n",
    "train_dataset['text'] = train_dataset['text'].apply(lambda x: remove_at(x))\r\n",
    "display(len(train_dataset))\r\n",
    "display(train_dataset['text'].head(5))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "0                    VirginAmerica What dhepburn said.\n",
       "1    VirginAmerica plus you've added commercials to...\n",
       "2    VirginAmerica I didn't today... Must mean I ne...\n",
       "3    VirginAmerica it's really aggressive to blast ...\n",
       "4    VirginAmerica and it's a really big bad thing ...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "#EXPLORATORY ANAYSIS\r\n",
    "#Checking if the dataset is balanced\r\n",
    "#extracting types of labels from the 'airline_sentiment' column in the dataset\r\n",
    "PosSentences = train_dataset[train_dataset['airline_sentiment'] == 'positive']\r\n",
    "NegSentences = train_dataset[train_dataset['airline_sentiment'] == 'negative']\r\n",
    "NeuSentences = train_dataset[train_dataset['airline_sentiment'] == 'neutral']\r\n",
    "\r\n",
    "#grouping all labels to find out the count for each group\r\n",
    "label_groups = train_dataset.groupby('airline_sentiment').size().reset_index(name='counts')\r\n",
    "display(label_groups.head(5))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>9178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>3099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  counts\n",
       "0          negative    9178\n",
       "1           neutral    3099\n",
       "2          positive    2363"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "train_dataset['airline_sentiment'].value_counts(normalize=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "negative    0.626913\n",
       "neutral     0.211680\n",
       "positive    0.161407\n",
       "Name: airline_sentiment, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "weights = {'negative':1, 'neutral':3, 'positive':4}\r\n",
    "train_dataset['weights'] = train_dataset['airline_sentiment'].apply(lambda x: weights[x])\r\n",
    "train_dataset['weights'].head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    3\n",
       "1    4\n",
       "2    3\n",
       "3    1\n",
       "4    1\n",
       "Name: weights, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "train_dataset = train_dataset.sample(n = 18000, weights=train_dataset['weights'], replace= True, random_state=24)\r\n",
    "train_dataset = train_dataset.reset_index()\r\n",
    "train_dataset['airline_sentiment'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "positive    6081\n",
       "neutral     5967\n",
       "negative    5952\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "#Visualizing label group data within training dataset\r\n",
    "# NOTE that the dataset is unbalanced\r\n",
    "# please come up with strategies to balance this dataset\r\n",
    "# if not, explain how that affect your results\r\n",
    "# if using unbalanced dataset, please provide MicroAverage evaluation scores from your classifier\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "label_groups = train_dataset.groupby('airline_sentiment').size().reset_index(name='counts')\r\n",
    "plt.figure(figsize=(8,5))\r\n",
    "ax = sns.barplot(x=\"airline_sentiment\", y=\"counts\", data=label_groups)\r\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\r\n",
    "ax.set_title(label=\"Sentiment Distribution\")\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# tokenizing using RegexTokenizer for tokenizing using specific regular expressions\r\n",
    "# in our case, we eliminate Twitter's noise by picking up only alphabets and numbers\r\n",
    "# indicated by the 'w', the regex alias for 'word'\r\n",
    "\r\n",
    "import nltk\r\n",
    "\r\n",
    "tokenizer = nltk.RegexpTokenizer('\\w+')\r\n",
    "doc = train_dataset['text'].apply(lambda x : tokenizer.tokenize(x))\r\n",
    "display(len(doc))\r\n",
    "display(doc.head(5))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "0    [AmericanAir, You, could, only, get, us, on, a...\n",
       "1    [USAirways, trying, to, book, award, travel, l...\n",
       "2    [AmericanAir, you, have, my, money, you, chang...\n",
       "3    [united, thanks, for, making, sure, they, hear...\n",
       "4    [SouthwestAir, thanks, What, s, your, opinion,...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "#BEGGINING TO WORK ON THE CLASSIFICATION TASK\r\n",
    "# extracting the tweet message and its asigned sentiment label\r\n",
    "# we do this by creating a Python list\r\n",
    "# our list will have the tweet's tokens and corresponding sentiments\r\n",
    "\r\n",
    "# opening a list to store our data\r\n",
    "docs = []\r\n",
    "\r\n",
    "# iterating over the dataset and extracting the information sought\r\n",
    "for i in range(0, len(train_dataset['airline_sentiment'])):\r\n",
    "    # appending the info to the list\r\n",
    "    docs.append((doc[i], train_dataset['airline_sentiment'][i]))\r\n",
    "\r\n",
    "#printing the output for validation\r\n",
    "docs[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['AmericanAir',\n",
       "  'You',\n",
       "  'could',\n",
       "  'only',\n",
       "  'get',\n",
       "  'us',\n",
       "  'on',\n",
       "  'a',\n",
       "  'flight',\n",
       "  '30',\n",
       "  'minutes',\n",
       "  'before',\n",
       "  'the',\n",
       "  'funeral',\n",
       "  'starts',\n",
       "  'We',\n",
       "  're',\n",
       "  'stranded',\n",
       "  'in',\n",
       "  'Dallas',\n",
       "  'w',\n",
       "  'o',\n",
       "  'luggage',\n",
       "  'for',\n",
       "  '2',\n",
       "  'days'],\n",
       " 'negative')"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "    \r\n",
    "# The dataset is sorted by sentiment label\r\n",
    "# so we need to randomize it to avoid sampling biases \r\n",
    "\r\n",
    "#importing the random library\r\n",
    "import random\r\n",
    "\r\n",
    "#shuffling the documents\r\n",
    "random.shuffle(docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "#defining set of words that will be used for features\r\n",
    "#we'll find the 2000 most common words and used them as an important feature of the whole corpus\r\n",
    "\r\n",
    "all_words = [word for (sentance,category) in docs for word in sentance]\r\n",
    "top_words = nltk.FreqDist(all_words)\r\n",
    "most_common_words = top_words.most_common(2000)\r\n",
    "word_features = [word for (word,count) in most_common_words]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "#checking if we have the 2000 words we need\r\n",
    "len(set(all_words))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13972"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "#now we will use that list of most frequent words in the entire corpus\r\n",
    "#to iterate over each sentence and check if any of those words are present\r\n",
    "#in that way, we will see if this unigram corpus feature is present on that particular sentence\r\n",
    "#using Boolean logic that matches values and returns 'True' or 'False'\r\n",
    "#we do this by defining a Python \"function,\" i.e.a piece of code writen to be reused\r\n",
    "def document_features(document, word_features):\r\n",
    "    document_words = set(document)\r\n",
    "    #we open a Pytnon dictionary instead of a list\r\n",
    "    features = {}\r\n",
    "    for word in word_features:\r\n",
    "        #checking if the word from word_features matches a word in the document\r\n",
    "        features['contains({})'.format(word)] = (word in document_words)\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "#now we apply the function to the document dataset\r\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in docs]\r\n",
    "\r\n",
    "#we print the list of features matches for the first document ([0]) in the corpus\r\n",
    "#we'll see a Python dictionary with the key being the feature word\r\n",
    "#and the value being 'True' or 'False' according to that word being matched in the present document or not\r\n",
    "#we'll se a lot of 'False' values because (of course) not all 2000 words will be on each sentence!\r\n",
    "featuresets[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'contains(to)': True,\n",
       "  'contains(I)': False,\n",
       "  'contains(the)': True,\n",
       "  'contains(you)': True,\n",
       "  'contains(a)': False,\n",
       "  'contains(for)': False,\n",
       "  'contains(united)': False,\n",
       "  'contains(on)': False,\n",
       "  'contains(t)': False,\n",
       "  'contains(and)': False,\n",
       "  'contains(SouthwestAir)': False,\n",
       "  'contains(JetBlue)': True,\n",
       "  'contains(flight)': False,\n",
       "  'contains(my)': False,\n",
       "  'contains(AmericanAir)': True,\n",
       "  'contains(USAirways)': False,\n",
       "  'contains(in)': True,\n",
       "  'contains(is)': True,\n",
       "  'contains(it)': False,\n",
       "  'contains(of)': False,\n",
       "  'contains(me)': False,\n",
       "  'contains(s)': False,\n",
       "  'contains(co)': False,\n",
       "  'contains(http)': False,\n",
       "  'contains(have)': False,\n",
       "  'contains(with)': False,\n",
       "  'contains(was)': False,\n",
       "  'contains(that)': False,\n",
       "  'contains(your)': False,\n",
       "  'contains(at)': False,\n",
       "  'contains(from)': True,\n",
       "  'contains(can)': True,\n",
       "  'contains(get)': False,\n",
       "  'contains(be)': False,\n",
       "  'contains(this)': False,\n",
       "  'contains(not)': False,\n",
       "  'contains(Thanks)': False,\n",
       "  'contains(but)': False,\n",
       "  'contains(thanks)': False,\n",
       "  'contains(are)': False,\n",
       "  'contains(so)': False,\n",
       "  'contains(service)': False,\n",
       "  'contains(we)': False,\n",
       "  'contains(no)': False,\n",
       "  'contains(do)': False,\n",
       "  'contains(an)': False,\n",
       "  'contains(Cancelled)': False,\n",
       "  'contains(just)': False,\n",
       "  'contains(now)': False,\n",
       "  'contains(VirginAmerica)': False,\n",
       "  'contains(will)': False,\n",
       "  'contains(help)': True,\n",
       "  'contains(2)': False,\n",
       "  'contains(out)': False,\n",
       "  'contains(amp)': False,\n",
       "  'contains(m)': False,\n",
       "  'contains(up)': False,\n",
       "  'contains(time)': False,\n",
       "  'contains(Flight)': False,\n",
       "  'contains(all)': False,\n",
       "  'contains(customer)': False,\n",
       "  'contains(Thank)': False,\n",
       "  'contains(flights)': False,\n",
       "  'contains(there)': False,\n",
       "  'contains(been)': False,\n",
       "  'contains(they)': False,\n",
       "  'contains(what)': False,\n",
       "  'contains(would)': False,\n",
       "  'contains(our)': False,\n",
       "  'contains(us)': False,\n",
       "  'contains(plane)': False,\n",
       "  'contains(back)': False,\n",
       "  'contains(if)': False,\n",
       "  'contains(when)': False,\n",
       "  'contains(thank)': False,\n",
       "  'contains(got)': False,\n",
       "  'contains(one)': False,\n",
       "  'contains(great)': False,\n",
       "  'contains(guys)': False,\n",
       "  'contains(need)': True,\n",
       "  'contains(about)': False,\n",
       "  'contains(please)': False,\n",
       "  'contains(today)': False,\n",
       "  'contains(You)': False,\n",
       "  'contains(as)': False,\n",
       "  'contains(like)': False,\n",
       "  'contains(or)': False,\n",
       "  'contains(had)': False,\n",
       "  'contains(still)': False,\n",
       "  'contains(know)': False,\n",
       "  'contains(has)': False,\n",
       "  'contains(gate)': False,\n",
       "  'contains(Flightled)': False,\n",
       "  'contains(fly)': False,\n",
       "  'contains(hours)': False,\n",
       "  'contains(any)': False,\n",
       "  'contains(hold)': False,\n",
       "  'contains(way)': False,\n",
       "  'contains(call)': False,\n",
       "  'contains(airline)': False,\n",
       "  'contains(bag)': False,\n",
       "  'contains(after)': False,\n",
       "  'contains(how)': False,\n",
       "  'contains(re)': False,\n",
       "  'contains(i)': False,\n",
       "  'contains(ve)': False,\n",
       "  'contains(Can)': False,\n",
       "  'contains(am)': False,\n",
       "  'contains(tomorrow)': False,\n",
       "  'contains(DM)': False,\n",
       "  'contains(ll)': False,\n",
       "  'contains(We)': False,\n",
       "  'contains(much)': False,\n",
       "  'contains(flying)': False,\n",
       "  'contains(love)': False,\n",
       "  'contains(don)': False,\n",
       "  'contains(why)': False,\n",
       "  'contains(hour)': False,\n",
       "  'contains(by)': False,\n",
       "  'contains(good)': False,\n",
       "  'contains(phone)': False,\n",
       "  'contains(more)': False,\n",
       "  'contains(1)': False,\n",
       "  'contains(change)': False,\n",
       "  'contains(United)': False,\n",
       "  'contains(day)': False,\n",
       "  'contains(delayed)': False,\n",
       "  'contains(home)': False,\n",
       "  'contains(3)': False,\n",
       "  'contains(go)': False,\n",
       "  'contains(Late)': False,\n",
       "  'contains(make)': False,\n",
       "  'contains(see)': False,\n",
       "  'contains(4)': False,\n",
       "  'contains(wait)': False,\n",
       "  'contains(again)': False,\n",
       "  'contains(airport)': False,\n",
       "  'contains(It)': False,\n",
       "  'contains(going)': False,\n",
       "  'contains(did)': False,\n",
       "  'contains(over)': False,\n",
       "  'contains(u)': False,\n",
       "  'contains(weather)': False,\n",
       "  'contains(next)': False,\n",
       "  'contains(crew)': False,\n",
       "  'contains(ticket)': False,\n",
       "  'contains(then)': False,\n",
       "  'contains(travel)': False,\n",
       "  'contains(best)': False,\n",
       "  'contains(were)': False,\n",
       "  'contains(should)': False,\n",
       "  'contains(take)': False,\n",
       "  'contains(here)': False,\n",
       "  'contains(check)': False,\n",
       "  'contains(only)': False,\n",
       "  'contains(new)': False,\n",
       "  'contains(last)': False,\n",
       "  'contains(trip)': False,\n",
       "  'contains(seat)': False,\n",
       "  'contains(first)': False,\n",
       "  'contains(number)': False,\n",
       "  'contains(off)': False,\n",
       "  'contains(too)': False,\n",
       "  'contains(through)': False,\n",
       "  'contains(response)': False,\n",
       "  'contains(want)': False,\n",
       "  'contains(No)': False,\n",
       "  'contains(work)': False,\n",
       "  'contains(agent)': False,\n",
       "  'contains(Just)': False,\n",
       "  'contains(email)': False,\n",
       "  'contains(really)': False,\n",
       "  'contains(them)': False,\n",
       "  'contains(very)': False,\n",
       "  'contains(her)': False,\n",
       "  'contains(trying)': False,\n",
       "  'contains(waiting)': False,\n",
       "  'contains(because)': False,\n",
       "  'contains(The)': False,\n",
       "  'contains(sent)': False,\n",
       "  'contains(w)': False,\n",
       "  'contains(getting)': False,\n",
       "  'contains(AA)': False,\n",
       "  'contains(people)': False,\n",
       "  'contains(My)': False,\n",
       "  'contains(minutes)': False,\n",
       "  'contains(What)': False,\n",
       "  'contains(book)': False,\n",
       "  'contains(Our)': False,\n",
       "  'contains(made)': False,\n",
       "  'contains(who)': False,\n",
       "  'contains(5)': False,\n",
       "  'contains(another)': False,\n",
       "  'contains(How)': False,\n",
       "  'contains(some)': False,\n",
       "  'contains(delay)': False,\n",
       "  'contains(could)': False,\n",
       "  'contains(bags)': False,\n",
       "  'contains(morning)': False,\n",
       "  'contains(ever)': False,\n",
       "  'contains(right)': False,\n",
       "  'contains(even)': False,\n",
       "  'contains(their)': False,\n",
       "  'contains(seats)': False,\n",
       "  'contains(JFK)': False,\n",
       "  'contains(Is)': False,\n",
       "  'contains(than)': False,\n",
       "  'contains(she)': False,\n",
       "  'contains(fleet)': False,\n",
       "  'contains(fleek)': False,\n",
       "  'contains(follow)': False,\n",
       "  'contains(baggage)': False,\n",
       "  'contains(d)': False,\n",
       "  'contains(luggage)': False,\n",
       "  'contains(boarding)': False,\n",
       "  'contains(yes)': False,\n",
       "  'contains(sure)': False,\n",
       "  'contains(before)': False,\n",
       "  'contains(Please)': False,\n",
       "  'contains(awesome)': False,\n",
       "  'contains(tonight)': False,\n",
       "  'contains(other)': False,\n",
       "  'contains(experience)': False,\n",
       "  'contains(This)': False,\n",
       "  'contains(due)': False,\n",
       "  'contains(amazing)': False,\n",
       "  'contains(online)': False,\n",
       "  'contains(passengers)': False,\n",
       "  'contains(never)': False,\n",
       "  'contains(didn)': False,\n",
       "  'contains(give)': False,\n",
       "  'contains(someone)': False,\n",
       "  'contains(better)': False,\n",
       "  'contains(days)': False,\n",
       "  'contains(left)': False,\n",
       "  'contains(lost)': False,\n",
       "  'contains(told)': False,\n",
       "  'contains(staff)': False,\n",
       "  'contains(Flighted)': False,\n",
       "  'contains(long)': False,\n",
       "  'contains(let)': False,\n",
       "  'contains(US)': False,\n",
       "  'contains(care)': False,\n",
       "  'contains(two)': False,\n",
       "  'contains(being)': False,\n",
       "  'contains(into)': False,\n",
       "  'contains(think)': False,\n",
       "  'contains(free)': False,\n",
       "  'contains(use)': False,\n",
       "  'contains(said)': False,\n",
       "  'contains(booked)': False,\n",
       "  'contains(appreciate)': False,\n",
       "  'contains(tickets)': False,\n",
       "  'contains(Booking)': False,\n",
       "  'contains(That)': False,\n",
       "  'contains(10)': False,\n",
       "  'contains(able)': False,\n",
       "  'contains(DFW)': False,\n",
       "  'contains(reservation)': False,\n",
       "  'contains(6)': False,\n",
       "  'contains(30)': False,\n",
       "  'contains(They)': True,\n",
       "  'contains(well)': False,\n",
       "  'contains(Your)': False,\n",
       "  'contains(DestinationDragons)': False,\n",
       "  'contains(same)': False,\n",
       "  'contains(Any)': False,\n",
       "  'contains(does)': False,\n",
       "  'contains(A)': False,\n",
       "  'contains(already)': False,\n",
       "  'contains(find)': False,\n",
       "  'contains(LAX)': False,\n",
       "  'contains(hrs)': False,\n",
       "  'contains(info)': False,\n",
       "  'contains(week)': False,\n",
       "  'contains(since)': False,\n",
       "  'contains(where)': False,\n",
       "  'contains(Problems)': False,\n",
       "  'contains(If)': False,\n",
       "  'contains(Great)': False,\n",
       "  'contains(doesn)': False,\n",
       "  'contains(airlines)': True,\n",
       "  'contains(tell)': False,\n",
       "  'contains(yet)': False,\n",
       "  'contains(job)': False,\n",
       "  'contains(ORD)': False,\n",
       "  'contains(he)': False,\n",
       "  'contains(via)': False,\n",
       "  'contains(down)': False,\n",
       "  'contains(Not)': False,\n",
       "  'contains(connection)': False,\n",
       "  'contains(min)': False,\n",
       "  'contains(Southwest)': False,\n",
       "  'contains(class)': False,\n",
       "  'contains(nice)': False,\n",
       "  'contains(But)': False,\n",
       "  'contains(RT)': False,\n",
       "  'contains(So)': False,\n",
       "  'contains(sitting)': False,\n",
       "  'contains(And)': False,\n",
       "  'contains(customers)': False,\n",
       "  'contains(miles)': False,\n",
       "  'contains(bad)': False,\n",
       "  'contains(Do)': False,\n",
       "  'contains(done)': False,\n",
       "  'contains(say)': False,\n",
       "  'contains(early)': False,\n",
       "  'contains(rebooked)': False,\n",
       "  'contains(hope)': False,\n",
       "  'contains(issue)': False,\n",
       "  'contains(SFO)': False,\n",
       "  'contains(Why)': False,\n",
       "  'contains(helpful)': False,\n",
       "  'contains(Dallas)': False,\n",
       "  'contains(working)': False,\n",
       "  'contains(PHL)': False,\n",
       "  'contains(won)': False,\n",
       "  'contains(stuck)': True,\n",
       "  'contains(ok)': False,\n",
       "  'contains(night)': False,\n",
       "  'contains(show)': False,\n",
       "  'contains(miss)': False,\n",
       "  'contains(put)': False,\n",
       "  'contains(name)': False,\n",
       "  'contains(upgrade)': False,\n",
       "  'contains(Will)': False,\n",
       "  'contains(line)': False,\n",
       "  'contains(says)': False,\n",
       "  'contains(ago)': False,\n",
       "  'contains(team)': False,\n",
       "  'contains(year)': False,\n",
       "  'contains(Imaginedragons)': False,\n",
       "  'contains(agents)': False,\n",
       "  'contains(checked)': False,\n",
       "  'contains(YOU)': False,\n",
       "  'contains(times)': False,\n",
       "  'contains(delays)': False,\n",
       "  'contains(website)': False,\n",
       "  'contains(refund)': False,\n",
       "  'contains(Chicago)': False,\n",
       "  'contains(always)': False,\n",
       "  'contains(chance)': False,\n",
       "  'contains(1st)': False,\n",
       "  'contains(flt)': False,\n",
       "  'contains(keep)': False,\n",
       "  'contains(though)': False,\n",
       "  'contains(making)': False,\n",
       "  'contains(board)': False,\n",
       "  'contains(nothing)': False,\n",
       "  'contains(add)': False,\n",
       "  'contains(send)': False,\n",
       "  'contains(called)': False,\n",
       "  'contains(planes)': False,\n",
       "  'contains(gt)': False,\n",
       "  'contains(worst)': False,\n",
       "  'contains(app)': False,\n",
       "  'contains(earlier)': False,\n",
       "  'contains(Vegas)': False,\n",
       "  'contains(8)': False,\n",
       "  'contains(update)': False,\n",
       "  'contains(quick)': False,\n",
       "  'contains(finally)': False,\n",
       "  'contains(air)': False,\n",
       "  'contains(Thx)': False,\n",
       "  'contains(open)': False,\n",
       "  'contains(snow)': False,\n",
       "  'contains(rebook)': False,\n",
       "  'contains(Flightr)': False,\n",
       "  'contains(7)': False,\n",
       "  'contains(attendant)': False,\n",
       "  'contains(start)': False,\n",
       "  'contains(its)': False,\n",
       "  'contains(look)': False,\n",
       "  'contains(happy)': False,\n",
       "  'contains(come)': False,\n",
       "  'contains(CLT)': False,\n",
       "  'contains(status)': False,\n",
       "  'contains(cool)': False,\n",
       "  'contains(issues)': False,\n",
       "  'contains(jetblue)': False,\n",
       "  'contains(claim)': False,\n",
       "  'contains(available)': False,\n",
       "  'contains(DCA)': False,\n",
       "  'contains(anything)': False,\n",
       "  'contains(soon)': False,\n",
       "  'contains(She)': False,\n",
       "  'contains(rep)': False,\n",
       "  'contains(having)': False,\n",
       "  'contains(family)': False,\n",
       "  'contains(taking)': False,\n",
       "  'contains(voucher)': False,\n",
       "  'contains(Love)': False,\n",
       "  'contains(BOS)': False,\n",
       "  'contains(reply)': False,\n",
       "  'contains(direct)': False,\n",
       "  'contains(ground)': False,\n",
       "  'contains(Hi)': False,\n",
       "  'contains(wifi)': False,\n",
       "  'contains(problem)': False,\n",
       "  'contains(Airways)': False,\n",
       "  'contains(credit)': False,\n",
       "  'contains(Now)': False,\n",
       "  'contains(pass)': False,\n",
       "  'contains(thing)': False,\n",
       "  'contains(20)': False,\n",
       "  'contains(pay)': False,\n",
       "  'contains(doing)': False,\n",
       "  'contains(When)': False,\n",
       "  'contains(person)': False,\n",
       "  'contains(many)': False,\n",
       "  'contains(American)': False,\n",
       "  'contains(Denver)': False,\n",
       "  'contains(his)': False,\n",
       "  'contains(few)': False,\n",
       "  'contains(15)': False,\n",
       "  'contains(which)': False,\n",
       "  'contains(tweet)': False,\n",
       "  'contains(things)': False,\n",
       "  'contains(usairways)': False,\n",
       "  'contains(business)': False,\n",
       "  'contains(site)': False,\n",
       "  'contains(appreciated)': False,\n",
       "  'contains(EWR)': False,\n",
       "  'contains(U)': False,\n",
       "  'contains(Boston)': False,\n",
       "  'contains(yesterday)': False,\n",
       "  'contains(Delta)': True,\n",
       "  'contains(while)': False,\n",
       "  'contains(anyone)': False,\n",
       "  'contains(system)': False,\n",
       "  'contains(coming)': False,\n",
       "  'contains(Yes)': False,\n",
       "  'contains(missed)': False,\n",
       "  'contains(missing)': False,\n",
       "  'contains(looking)': False,\n",
       "  'contains(pilot)': False,\n",
       "  'contains(NYC)': False,\n",
       "  'contains(also)': False,\n",
       "  'contains(On)': False,\n",
       "  'contains(food)': False,\n",
       "  'contains(hear)': False,\n",
       "  'contains(instead)': False,\n",
       "  'contains(mins)': False,\n",
       "  'contains(until)': False,\n",
       "  'contains(Need)': False,\n",
       "  'contains(Help)': False,\n",
       "  'contains(tried)': False,\n",
       "  'contains(landing)': False,\n",
       "  'contains(account)': False,\n",
       "  'contains(isn)': False,\n",
       "  'contains(something)': False,\n",
       "  'contains(away)': False,\n",
       "  'contains(wasn)': False,\n",
       "  'contains(Are)': False,\n",
       "  'contains(https)': False,\n",
       "  'contains(Have)': False,\n",
       "  'contains(landed)': False,\n",
       "  'contains(departure)': False,\n",
       "  'contains(received)': False,\n",
       "  'contains(confirmation)': False,\n",
       "  'contains(New)': False,\n",
       "  'contains(understand)': False,\n",
       "  'contains(every)': False,\n",
       "  'contains(contact)': False,\n",
       "  'contains(stop)': False,\n",
       "  'contains(San)': False,\n",
       "  'contains(card)': False,\n",
       "  'contains(y)': False,\n",
       "  'contains(car)': False,\n",
       "  'contains(hotel)': False,\n",
       "  'contains(forward)': False,\n",
       "  'contains(connecting)': False,\n",
       "  'contains(wife)': False,\n",
       "  'contains(Charlotte)': False,\n",
       "  'contains(9)': False,\n",
       "  'contains(points)': False,\n",
       "  'contains(took)': False,\n",
       "  'contains(try)': False,\n",
       "  'contains(leave)': False,\n",
       "  'contains(around)': False,\n",
       "  'contains(extra)': False,\n",
       "  'contains(11)': False,\n",
       "  'contains(rude)': False,\n",
       "  'contains(Newark)': False,\n",
       "  'contains(leaving)': False,\n",
       "  'contains(most)': False,\n",
       "  'contains(date)': False,\n",
       "  'contains(possible)': False,\n",
       "  'contains(24)': False,\n",
       "  'contains(UA)': False,\n",
       "  'contains(return)': False,\n",
       "  'contains(policy)': False,\n",
       "  'contains(CEO)': False,\n",
       "  'contains(far)': False,\n",
       "  'contains(friend)': False,\n",
       "  'contains(old)': False,\n",
       "  'contains(THE)': False,\n",
       "  'contains(25)': False,\n",
       "  'contains(pilots)': False,\n",
       "  'contains(sit)': False,\n",
       "  'contains(went)': False,\n",
       "  'contains(FLL)': False,\n",
       "  'contains(reason)': False,\n",
       "  'contains(talk)': False,\n",
       "  'contains(enough)': False,\n",
       "  'contains(link)': False,\n",
       "  'contains(during)': False,\n",
       "  'contains(speak)': False,\n",
       "  'contains(without)': False,\n",
       "  'contains(vacation)': False,\n",
       "  'contains(different)': False,\n",
       "  'contains(found)': False,\n",
       "  'contains(needs)': False,\n",
       "  'contains(least)': False,\n",
       "  'contains(southwestair)': False,\n",
       "  'contains(50)': False,\n",
       "  'contains(Sunday)': False,\n",
       "  'contains(question)': False,\n",
       "  'contains(actually)': False,\n",
       "  'contains(haven)': False,\n",
       "  'contains(Was)': False,\n",
       "  'contains(using)': False,\n",
       "  'contains(PHX)': False,\n",
       "  'contains(Good)': False,\n",
       "  'contains(employees)': False,\n",
       "  'contains(everything)': False,\n",
       "  'contains(kids)': False,\n",
       "  'contains(money)': False,\n",
       "  'contains(together)': False,\n",
       "  'contains(answer)': False,\n",
       "  'contains(him)': False,\n",
       "  'contains(weeks)': False,\n",
       "  'contains(these)': False,\n",
       "  'contains(reservations)': False,\n",
       "  'contains(company)': False,\n",
       "  'contains(destination)': False,\n",
       "  'contains(desk)': False,\n",
       "  'contains(40)': False,\n",
       "  'contains(45)': False,\n",
       "  'contains(LA)': False,\n",
       "  'contains(big)': False,\n",
       "  'contains(DC)': False,\n",
       "  'contains(those)': False,\n",
       "  'contains(sorry)': False,\n",
       "  'contains(changed)': False,\n",
       "  'contains(IAD)': False,\n",
       "  'contains(lot)': False,\n",
       "  'contains(LGA)': False,\n",
       "  'contains(end)': False,\n",
       "  'contains(Airport)': False,\n",
       "  'contains(All)': False,\n",
       "  'contains(full)': False,\n",
       "  'contains(point)': False,\n",
       "  'contains(thought)': False,\n",
       "  'contains(following)': False,\n",
       "  'contains(There)': False,\n",
       "  'contains(gave)': False,\n",
       "  'contains(message)': False,\n",
       "  'contains(lol)': False,\n",
       "  'contains(idea)': False,\n",
       "  'contains(SW)': False,\n",
       "  'contains(fee)': False,\n",
       "  'contains(both)': False,\n",
       "  'contains(life)': False,\n",
       "  'contains(may)': False,\n",
       "  'contains(Very)': False,\n",
       "  'contains(Looking)': False,\n",
       "  'contains(Atlanta)': False,\n",
       "  'contains(world)': False,\n",
       "  'contains(thru)': False,\n",
       "  'contains(hard)': False,\n",
       "  'contains(SWA)': False,\n",
       "  'contains(makes)': False,\n",
       "  'contains(stay)': False,\n",
       "  'contains(couldn)': False,\n",
       "  'contains(almost)': False,\n",
       "  'contains(little)': False,\n",
       "  'contains(He)': False,\n",
       "  'contains(helping)': False,\n",
       "  'contains(everyone)': False,\n",
       "  'contains(guy)': False,\n",
       "  'contains(Airlines)': False,\n",
       "  'contains(wonderful)': False,\n",
       "  'contains(past)': False,\n",
       "  'contains(rock)': False,\n",
       "  'contains(saying)': False,\n",
       "  'contains(charge)': False,\n",
       "  'contains(scheduled)': False,\n",
       "  'contains(Austin)': False,\n",
       "  'contains(Customer)': False,\n",
       "  'contains(ask)': False,\n",
       "  'contains(used)': False,\n",
       "  'contains(paid)': False,\n",
       "  'contains(looks)': False,\n",
       "  'contains(attendants)': False,\n",
       "  'contains(Awesome)': False,\n",
       "  'contains(Air)': False,\n",
       "  'contains(glad)': False,\n",
       "  'contains(lt)': False,\n",
       "  'contains(supposed)': False,\n",
       "  'contains(Nashville)': False,\n",
       "  'contains(thx)': False,\n",
       "  'contains(might)': False,\n",
       "  'contains(Twitter)': False,\n",
       "  'contains(seem)': False,\n",
       "  'contains(Still)': False,\n",
       "  'contains(12)': False,\n",
       "  'contains(plus)': False,\n",
       "  'contains(checking)': False,\n",
       "  'contains(ARE)': False,\n",
       "  'contains(Monday)': False,\n",
       "  'contains(ur)': False,\n",
       "  'contains(terminal)': False,\n",
       "  'contains(horrible)': False,\n",
       "  'contains(Well)': False,\n",
       "  'contains(assistance)': False,\n",
       "  'contains(process)': False,\n",
       "  'contains(request)': False,\n",
       "  'contains(Never)': False,\n",
       "  'contains(fees)': False,\n",
       "  'contains(March)': False,\n",
       "  'contains(mean)': False,\n",
       "  'contains(hung)': False,\n",
       "  'contains(changes)': False,\n",
       "  'contains(month)': False,\n",
       "  'contains(Don)': False,\n",
       "  'contains(bring)': False,\n",
       "  'contains(Really)': False,\n",
       "  'contains(member)': False,\n",
       "  'contains(disappointed)': False,\n",
       "  'contains(reFlight)': False,\n",
       "  'contains(Got)': False,\n",
       "  'contains(fix)': False,\n",
       "  'contains(Wall)': False,\n",
       "  'contains(b)': False,\n",
       "  'contains(under)': False,\n",
       "  'contains(happens)': False,\n",
       "  'contains(offer)': False,\n",
       "  'contains(bc)': False,\n",
       "  'contains(flew)': False,\n",
       "  'contains(calls)': False,\n",
       "  'contains(second)': False,\n",
       "  'contains(wanted)': False,\n",
       "  'contains(oh)': False,\n",
       "  'contains(Only)': False,\n",
       "  'contains(option)': False,\n",
       "  'contains(such)': False,\n",
       "  'contains(happened)': False,\n",
       "  'contains(each)': False,\n",
       "  'contains(arrived)': False,\n",
       "  'contains(MCO)': False,\n",
       "  'contains(cold)': False,\n",
       "  'contains(live)': False,\n",
       "  'contains(mechanical)': False,\n",
       "  'contains(feel)': False,\n",
       "  'contains(PLEASE)': False,\n",
       "  'contains(real)': True,\n",
       "  'contains(fail)': False,\n",
       "  'contains(Gate)': False,\n",
       "  'contains(southwest)': False,\n",
       "  'contains(running)': False,\n",
       "  'contains(stranded)': False,\n",
       "  'contains(twice)': False,\n",
       "  'contains(keeping)': False,\n",
       "  'contains(cost)': False,\n",
       "  'contains(fault)': False,\n",
       "  'contains(wrong)': True,\n",
       "  'contains(Let)': False,\n",
       "  'contains(years)': False,\n",
       "  'contains(guess)': False,\n",
       "  'contains(easy)': False,\n",
       "  'contains(carry)': False,\n",
       "  'contains(goes)': False,\n",
       "  'contains(Appreciate)': False,\n",
       "  'contains(terrible)': False,\n",
       "  'contains(100)': False,\n",
       "  'contains(C)': False,\n",
       "  'contains(In)': False,\n",
       "  'contains(route)': False,\n",
       "  'contains(excellent)': False,\n",
       "  'contains(favorite)': False,\n",
       "  'contains(TSA)': False,\n",
       "  'contains(ME)': False,\n",
       "  'contains(Philly)': False,\n",
       "  'contains(beyond)': False,\n",
       "  'contains(wish)': False,\n",
       "  'contains(form)': False,\n",
       "  'contains(DEN)': False,\n",
       "  'contains(believe)': False,\n",
       "  'contains(Been)': False,\n",
       "  'contains(ride)': False,\n",
       "  'contains(expect)': False,\n",
       "  'contains(super)': False,\n",
       "  'contains(helped)': False,\n",
       "  'contains(2015)': False,\n",
       "  'contains(else)': False,\n",
       "  'contains(AM)': False,\n",
       "  'contains(once)': False,\n",
       "  'contains(BWI)': False,\n",
       "  'contains(ready)': False,\n",
       "  'contains(loyal)': False,\n",
       "  'contains(Orlando)': False,\n",
       "  'contains(correct)': False,\n",
       "  'contains(daughter)': False,\n",
       "  'contains(Flightlations)': False,\n",
       "  'contains(2nd)': False,\n",
       "  'contains(traveling)': False,\n",
       "  'contains(row)': False,\n",
       "  'contains(warm)': False,\n",
       "  'contains(high)': False,\n",
       "  'contains(apology)': False,\n",
       "  'contains(Another)': False,\n",
       "  'contains(record)': False,\n",
       "  'contains(avgeek)': False,\n",
       "  'contains(BEST)': False,\n",
       "  'contains(set)': False,\n",
       "  'contains(hoping)': False,\n",
       "  'contains(rather)': False,\n",
       "  'contains(Would)': False,\n",
       "  'contains(leg)': False,\n",
       "  'contains(hey)': False,\n",
       "  'contains(flown)': False,\n",
       "  'contains(award)': False,\n",
       "  'contains(problems)': False,\n",
       "  'contains(dm)': False,\n",
       "  'contains(hi)': False,\n",
       "  'contains(address)': False,\n",
       "  'contains(part)': False,\n",
       "  'contains(room)': False,\n",
       "  'contains(calling)': False,\n",
       "  'contains(share)': False,\n",
       "  'contains(ATL)': False,\n",
       "  'contains(afternoon)': False,\n",
       "  'contains(deal)': False,\n",
       "  'contains(worries)': False,\n",
       "  'contains(IAH)': False,\n",
       "  'contains(LAS)': False,\n",
       "  'contains(happen)': False,\n",
       "  'contains(Im)': False,\n",
       "  'contains(half)': False,\n",
       "  'contains(Friday)': False,\n",
       "  'contains(less)': False,\n",
       "  'contains(non)': False,\n",
       "  'contains(support)': False,\n",
       "  'contains(plans)': False,\n",
       "  'contains(Keep)': False,\n",
       "  'contains(ridiculous)': False,\n",
       "  'contains(o)': False,\n",
       "  'contains(taken)': False,\n",
       "  'contains(Miami)': False,\n",
       "  'contains(THANK)': False,\n",
       "  'contains(TO)': False,\n",
       "  'contains(buy)': False,\n",
       "  'contains(despite)': False,\n",
       "  'contains(c)': False,\n",
       "  'contains(schedule)': False,\n",
       "  'contains(updates)': False,\n",
       "  'contains(hr)': False,\n",
       "  'contains(fun)': False,\n",
       "  'contains(D)': False,\n",
       "  'contains(frustrated)': False,\n",
       "  'contains(kind)': False,\n",
       "  'contains(situation)': False,\n",
       "  'contains(between)': False,\n",
       "  'contains(domestic)': False,\n",
       "  'contains(group)': False,\n",
       "  'contains(Flying)': False,\n",
       "  'contains(aren)': False,\n",
       "  'contains(longer)': False,\n",
       "  'contains(okay)': False,\n",
       "  'contains(awful)': False,\n",
       "  'contains(Journal)': False,\n",
       "  'contains(pick)': False,\n",
       "  'contains(runway)': False,\n",
       "  'contains(broken)': False,\n",
       "  'contains(mom)': False,\n",
       "  'contains(own)': False,\n",
       "  'contains(standby)': False,\n",
       "  'contains(child)': False,\n",
       "  'contains(saw)': False,\n",
       "  'contains(safety)': False,\n",
       "  'contains(win)': False,\n",
       "  'contains(Flighting)': False,\n",
       "  'contains(heard)': False,\n",
       "  'contains(changing)': False,\n",
       "  'contains(case)': False,\n",
       "  'contains(months)': False,\n",
       "  'contains(Houston)': False,\n",
       "  'contains(pretty)': False,\n",
       "  'contains(checkin)': False,\n",
       "  'contains(access)': False,\n",
       "  'contains(800)': False,\n",
       "  'contains(Maybe)': False,\n",
       "  'contains(Service)': False,\n",
       "  'contains(man)': False,\n",
       "  'contains(employee)': False,\n",
       "  'contains(details)': False,\n",
       "  'contains(minute)': False,\n",
       "  'contains(aircraft)': False,\n",
       "  'contains(yall)': False,\n",
       "  'contains(bought)': False,\n",
       "  'contains(birthday)': False,\n",
       "  'contains(excited)': False,\n",
       "  'contains(future)': False,\n",
       "  'contains(Tuesday)': False,\n",
       "  'contains(r)': False,\n",
       "  'contains(seating)': False,\n",
       "  'contains(drive)': False,\n",
       "  'contains(others)': False,\n",
       "  'contains(waited)': False,\n",
       "  'contains(Had)': False,\n",
       "  'contains(allowed)': False,\n",
       "  'contains(200)': False,\n",
       "  'contains(word)': False,\n",
       "  'contains(23)': False,\n",
       "  'contains(Next)': False,\n",
       "  'contains(Hey)': False,\n",
       "  'contains(son)': False,\n",
       "  'contains(suck)': False,\n",
       "  'contains(counter)': False,\n",
       "  'contains(arrive)': False,\n",
       "  'contains(Should)': False,\n",
       "  'contains(Trying)': False,\n",
       "  'contains(international)': False,\n",
       "  'contains(needed)': False,\n",
       "  'contains(moved)': False,\n",
       "  'contains(AUS)': False,\n",
       "  'contains(area)': False,\n",
       "  'contains(computer)': False,\n",
       "  'contains(BNA)': False,\n",
       "  'contains(customerservice)': False,\n",
       "  'contains(three)': False,\n",
       "  'contains(joke)': False,\n",
       "  'contains(price)': False,\n",
       "  'contains(works)': False,\n",
       "  'contains(fixed)': False,\n",
       "  'contains(worse)': False,\n",
       "  'contains(order)': False,\n",
       "  'contains(lose)': False,\n",
       "  'contains(lovely)': False,\n",
       "  'contains(provide)': False,\n",
       "  'contains(gets)': False,\n",
       "  'contains(90)': False,\n",
       "  'contains(NO)': False,\n",
       "  'contains(Who)': False,\n",
       "  'contains(luck)': False,\n",
       "  'contains(code)': False,\n",
       "  'contains(Nice)': False,\n",
       "  'contains(explain)': False,\n",
       "  'contains(passbook)': False,\n",
       "  'contains(weekend)': False,\n",
       "  'contains(Did)': False,\n",
       "  'contains(folks)': False,\n",
       "  'contains(reach)': False,\n",
       "  'contains(handle)': False,\n",
       "  'contains(information)': False,\n",
       "  'contains(place)': False,\n",
       "  'contains(Could)': False,\n",
       "  'contains(companion)': False,\n",
       "  'contains(americanair)': False,\n",
       "  'contains(mine)': False,\n",
       "  'contains(purchase)': False,\n",
       "  'contains(blue)': False,\n",
       "  'contains(responding)': False,\n",
       "  'contains(evening)': False,\n",
       "  'contains(services)': False,\n",
       "  'contains(top)': False,\n",
       "  'contains(run)': False,\n",
       "  'contains(sucks)': False,\n",
       "  'contains(priority)': False,\n",
       "  'contains(seriously)': False,\n",
       "  'contains(PDX)': False,\n",
       "  'contains(First)': False,\n",
       "  'contains(wouldn)': False,\n",
       "  'contains(000)': False,\n",
       "  'contains(came)': False,\n",
       "  'contains(absolutely)': False,\n",
       "  'contains(unacceptable)': False,\n",
       "  'contains(After)': False,\n",
       "  'contains(virginamerica)': False,\n",
       "  'contains(cannot)': False,\n",
       "  'contains(Going)': False,\n",
       "  'contains(options)': False,\n",
       "  'contains(page)': False,\n",
       "  'contains(round)': False,\n",
       "  'contains(jet)': False,\n",
       "  'contains(move)': False,\n",
       "  'contains(middle)': False,\n",
       "  'contains(office)': False,\n",
       "  'contains(TV)': False,\n",
       "  'contains(paying)': False,\n",
       "  'contains(Virgin)': False,\n",
       "  'contains(friends)': False,\n",
       "  'contains(true)': False,\n",
       "  'contains(telling)': False,\n",
       "  'contains(layover)': False,\n",
       "  'contains(several)': False,\n",
       "  'contains(Sorry)': False,\n",
       "  'contains(media)': False,\n",
       "  'contains(Columbus)': False,\n",
       "  'contains(Best)': False,\n",
       "  'contains(sending)': False,\n",
       "  'contains(haha)': False,\n",
       "  'contains(currently)': False,\n",
       "  'contains(connect)': False,\n",
       "  'contains(drop)': False,\n",
       "  'contains(Oh)': False,\n",
       "  'contains(respond)': False,\n",
       "  'contains(sleep)': False,\n",
       "  'contains(communication)': False,\n",
       "  'contains(safely)': False,\n",
       "  'contains(Hopefully)': False,\n",
       "  'contains(deals)': False,\n",
       "  'contains(raise)': False,\n",
       "  'contains(poor)': False,\n",
       "  'contains(catch)': False,\n",
       "  'contains(USAir)': False,\n",
       "  'contains(frustrating)': False,\n",
       "  'contains(Also)': False,\n",
       "  'contains(Passbook)': False,\n",
       "  'contains(asking)': False,\n",
       "  'contains(cust)': False,\n",
       "  'contains(One)': False,\n",
       "  'contains(error)': False,\n",
       "  'contains(winter)': False,\n",
       "  'contains(WSJ)': False,\n",
       "  'contains(empty)': False,\n",
       "  'contains(offered)': False,\n",
       "  'contains(baby)': False,\n",
       "  'contains(original)': False,\n",
       "  'contains(whole)': False,\n",
       "  'contains(small)': False,\n",
       "  'contains(S)': False,\n",
       "  'contains(party)': False,\n",
       "  'contains(fare)': False,\n",
       "  'contains(luv)': False,\n",
       "  'contains(watch)': False,\n",
       "  'contains(sad)': False,\n",
       "  'contains(feedback)': False,\n",
       "  'contains(maybe)': False,\n",
       "  'contains(Feb)': False,\n",
       "  'contains(maintenance)': False,\n",
       "  'contains(safe)': False,\n",
       "  'contains(Street)': False,\n",
       "  'contains(cabin)': False,\n",
       "  'contains(iPhone)': False,\n",
       "  'contains(connections)': False,\n",
       "  'contains(knows)': False,\n",
       "  'contains(SO)': False,\n",
       "  'contains(DAL)': False,\n",
       "  'contains(space)': False,\n",
       "  'contains(Flightlation)': False,\n",
       "  'contains(See)': False,\n",
       "  'contains(takes)': False,\n",
       "  'contains(given)': False,\n",
       "  'contains(Flights)': False,\n",
       "  'contains(giving)': False,\n",
       "  'contains(gonna)': False,\n",
       "  'contains(impressed)': False,\n",
       "  'contains(HELP)': False,\n",
       "  'contains(cause)': False,\n",
       "  'contains(knew)': False,\n",
       "  'contains(country)': False,\n",
       "  'contains(ABQ)': False,\n",
       "  'contains(ice)': False,\n",
       "  'contains(storm)': False,\n",
       "  'contains(social)': False,\n",
       "  'contains(fares)': False,\n",
       "  'contains(deserves)': False,\n",
       "  'contains(short)': False,\n",
       "  'contains(traveler)': False,\n",
       "  'contains(perfect)': False,\n",
       "  'contains(hopefully)': False,\n",
       "  'contains(passenger)': False,\n",
       "  'contains(hate)': False,\n",
       "  'contains(seen)': False,\n",
       "  'contains(UnitedAirlines)': False,\n",
       "  'contains(crews)': False,\n",
       "  'contains(35)': False,\n",
       "  'contains(At)': False,\n",
       "  'contains(America)': False,\n",
       "  'contains(anyway)': False,\n",
       "  'contains(twitter)': False,\n",
       "  'contains(MY)': False,\n",
       "  'contains(FOLLOW)': False,\n",
       "  'contains(mobile)': False,\n",
       "  'contains(Everyone)': False,\n",
       "  'contains(huge)': False,\n",
       "  'contains(members)': False,\n",
       "  'contains(item)': False,\n",
       "  'contains(post)': False,\n",
       "  'contains(dont)': False,\n",
       "  'contains(fan)': False,\n",
       "  'contains(busy)': False,\n",
       "  'contains(wow)': False,\n",
       "  'contains(22)': False,\n",
       "  'contains(receive)': False,\n",
       "  'contains(DIA)': False,\n",
       "  'contains(definitely)': False,\n",
       "  'contains(Time)': False,\n",
       "  ...},\n",
       " 'negative')"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "#checking the length of the list of features\r\n",
    "len(featuresets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "#We are ready now to do machine learning using the unigram list we just created\r\n",
    "#we use a Naive Bayes classifier with 5-fold cross validation for training on sentiments using unigrams\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "kf = KFold(n_splits = 10)\r\n",
    "sum = 0\r\n",
    "\r\n",
    "for train, test in kf.split(featuresets):\r\n",
    "    train_data = np.array(featuresets)[train]\r\n",
    "    test_data = np.array(featuresets)[test]\r\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_data)\r\n",
    "    sum += nltk.classify.accuracy(classifier, test_data)\r\n",
    "\r\n",
    "#storing the score in a variable \r\n",
    "acc1 = sum/10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "#let's see the accuracy score for this unigram classifier\r\n",
    "\r\n",
    "acc1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7695000000000001"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# We will now create a new feature: bigrams\r\n",
    "#we'll use the code we already know from class labs \r\n",
    "\r\n",
    "from nltk.collocations import *\r\n",
    "import re\r\n",
    "\r\n",
    "#data cleaning and preprocessing\r\n",
    "stopwords = nltk.corpus.stopwords.words('english')\r\n",
    "\r\n",
    "def alpha(w):\r\n",
    "    pattern = re.compile('^[^a-z]+$')\r\n",
    "    if(pattern.match(w)):\r\n",
    "        return True\r\n",
    "    else:\r\n",
    "        return False\r\n",
    "\r\n",
    "#creating bigrams features for the corpus and applying cleaning steps\r\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\r\n",
    "finder = BigramCollocationFinder.from_words(all_words)\r\n",
    "finder.apply_word_filter(alpha)\r\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\r\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\r\n",
    "scored[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('customer', 'service'), 0.001836721137239272),\n",
       " (('Cancelled', 'Flightled'), 0.0015145476285372658),\n",
       " (('Late', 'Flight'), 0.0007008104158363226),\n",
       " (('JetBlue', 'Our'), 0.0006842035339444666),\n",
       " (('Our', 'fleet'), 0.0006775607811877242),\n",
       " (('fleek', 'http'), 0.0006709180284309818),\n",
       " (('Cancelled', 'Flighted'), 0.0005845622425933307),\n",
       " (('JetBlue', 'thanks'), 0.0005114919622691643),\n",
       " (('Booking', 'Problems'), 0.0004982064567556796),\n",
       " (('united', 'thanks'), 0.0004915637039989371)]"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "#extracting clean bigrams (no frequency information)\r\n",
    "bigram_features = [bigram for (bigram, count) in scored[:2000]]\r\n",
    "#printing the first 30 for confirmation\r\n",
    "bigram_features[:30]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('customer', 'service'),\n",
       " ('Cancelled', 'Flightled'),\n",
       " ('Late', 'Flight'),\n",
       " ('JetBlue', 'Our'),\n",
       " ('Our', 'fleet'),\n",
       " ('fleek', 'http'),\n",
       " ('Cancelled', 'Flighted'),\n",
       " ('JetBlue', 'thanks'),\n",
       " ('Booking', 'Problems'),\n",
       " ('united', 'thanks'),\n",
       " ('Cancelled', 'Flight'),\n",
       " ('SouthwestAir', 'thanks'),\n",
       " ('AmericanAir', 'thanks'),\n",
       " ('Late', 'Flightr'),\n",
       " ('Flight', 'Booking'),\n",
       " ('USAirways', 'AmericanAir'),\n",
       " ('SouthwestAir', 'Thanks'),\n",
       " ('united', 'thank'),\n",
       " ('gate', 'agent'),\n",
       " ('USAirways', 'thanks'),\n",
       " ('united', 'Thanks'),\n",
       " ('JetBlue', 'thank'),\n",
       " ('SouthwestAir', 'thank'),\n",
       " ('flight', 'attendant'),\n",
       " ('great', 'flight'),\n",
       " ('Thanks', 'united'),\n",
       " ('You', 'guys'),\n",
       " ('JetBlue', 'Thanks'),\n",
       " ('get', 'home'),\n",
       " ('Thanks', 'SouthwestAir')]"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "#after finding all bigrams of the corpus (i.e. the bigram feature of the corpus)\r\n",
    "#we create a function that checks if those feature bigrams are present on each specific document\r\n",
    "#exactly as we did with unigrams\r\n",
    "\r\n",
    "def bi_document_features(document, bigram_features):\r\n",
    "    document_words = list(nltk.bigrams(document))\r\n",
    "    features = {}\r\n",
    "    for word in bigram_features:\r\n",
    "        #boolean logic will retunt 'True' if there is a match, or 'False' if not\r\n",
    "        features['contains({})'.format(word)] = (word in document_words)\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "#applying the function to our documents\r\n",
    "featuresets2 = [(bi_document_features(d, bigram_features), c) for (d, c) in docs]\r\n",
    "\r\n",
    "#seeing the featureset for the first document\r\n",
    "featuresets2[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({\"contains(('customer', 'service'))\": False,\n",
       "  \"contains(('Cancelled', 'Flightled'))\": False,\n",
       "  \"contains(('Late', 'Flight'))\": False,\n",
       "  \"contains(('JetBlue', 'Our'))\": False,\n",
       "  \"contains(('Our', 'fleet'))\": False,\n",
       "  \"contains(('fleek', 'http'))\": False,\n",
       "  \"contains(('Cancelled', 'Flighted'))\": False,\n",
       "  \"contains(('JetBlue', 'thanks'))\": False,\n",
       "  \"contains(('Booking', 'Problems'))\": False,\n",
       "  \"contains(('united', 'thanks'))\": False,\n",
       "  \"contains(('Cancelled', 'Flight'))\": False,\n",
       "  \"contains(('SouthwestAir', 'thanks'))\": False,\n",
       "  \"contains(('AmericanAir', 'thanks'))\": False,\n",
       "  \"contains(('Late', 'Flightr'))\": False,\n",
       "  \"contains(('Flight', 'Booking'))\": False,\n",
       "  \"contains(('USAirways', 'AmericanAir'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Thanks'))\": False,\n",
       "  \"contains(('united', 'thank'))\": False,\n",
       "  \"contains(('gate', 'agent'))\": False,\n",
       "  \"contains(('USAirways', 'thanks'))\": False,\n",
       "  \"contains(('united', 'Thanks'))\": False,\n",
       "  \"contains(('JetBlue', 'thank'))\": False,\n",
       "  \"contains(('SouthwestAir', 'thank'))\": False,\n",
       "  \"contains(('flight', 'attendant'))\": False,\n",
       "  \"contains(('great', 'flight'))\": False,\n",
       "  \"contains(('Thanks', 'united'))\": False,\n",
       "  \"contains(('You', 'guys'))\": False,\n",
       "  \"contains(('JetBlue', 'Thanks'))\": False,\n",
       "  \"contains(('get', 'home'))\": False,\n",
       "  \"contains(('Thanks', 'SouthwestAir'))\": False,\n",
       "  \"contains(('united', 'Thank'))\": False,\n",
       "  \"contains(('thanks', 'united'))\": False,\n",
       "  \"contains(('AmericanAir', 'Thank'))\": False,\n",
       "  \"contains(('Thanks', 'JetBlue'))\": False,\n",
       "  \"contains(('connecting', 'flight'))\": False,\n",
       "  \"contains(('first', 'class'))\": False,\n",
       "  \"contains(('AmericanAir', 'Thanks'))\": False,\n",
       "  \"contains(('call', 'back'))\": False,\n",
       "  \"contains(('help', 'united'))\": False,\n",
       "  \"contains(('reFlight', 'Booking'))\": False,\n",
       "  \"contains(('Thanks', 'AmericanAir'))\": False,\n",
       "  \"contains(('next', 'flight'))\": False,\n",
       "  \"contains(('united', 'yes'))\": False,\n",
       "  \"contains(('Thanks', 'USAirways'))\": False,\n",
       "  \"contains(('flight', 'SouthwestAir'))\": False,\n",
       "  \"contains(('quick', 'response'))\": False,\n",
       "  \"contains(('would', 'love'))\": False,\n",
       "  \"contains(('JetBlue', 'Thank'))\": False,\n",
       "  \"contains(('AmericanAir', 'thank'))\": False,\n",
       "  \"contains(('Cancelled', 'Flightlations'))\": False,\n",
       "  \"contains(('first', 'time'))\": False,\n",
       "  \"contains(('baggage', 'claim'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Thank'))\": False,\n",
       "  \"contains(('USAirways', 'thank'))\": False,\n",
       "  \"contains(('flight', 'Cancelled'))\": False,\n",
       "  \"contains(('looks', 'like'))\": False,\n",
       "  \"contains(('AmericanAir', 'Flight'))\": False,\n",
       "  \"contains(('flight', 'united'))\": False,\n",
       "  \"contains(('united', 'flight'))\": False,\n",
       "  \"contains(('wait', 'time'))\": False,\n",
       "  \"contains(('AmericanAir', 'You'))\": False,\n",
       "  \"contains(('Cancelled', 'Flighting'))\": False,\n",
       "  \"contains(('another', 'flight'))\": False,\n",
       "  \"contains(('boarding', 'pass'))\": False,\n",
       "  \"contains(('flight', 'attendants'))\": False,\n",
       "  \"contains(('hour', 'delay'))\": False,\n",
       "  \"contains(('hours', 'Late'))\": False,\n",
       "  \"contains(('thanks', 'SouthwestAir'))\": False,\n",
       "  \"contains(('earlier', 'flight'))\": False,\n",
       "  \"contains(('thanks', 'AmericanAir'))\": False,\n",
       "  \"contains(('tomorrow', 'morning'))\": False,\n",
       "  \"contains(('USAirways', 'Thank'))\": False,\n",
       "  \"contains(('help', 'AmericanAir'))\": False,\n",
       "  \"contains(('last', 'night'))\": False,\n",
       "  \"contains(('would', 'like'))\": False,\n",
       "  \"contains(('JetBlue', 'Airways'))\": False,\n",
       "  \"contains(('flight', 'crew'))\": False,\n",
       "  \"contains(('thanks', 'USAirways'))\": False,\n",
       "  \"contains(('1st', 'class'))\": False,\n",
       "  \"contains(('checked', 'bag'))\": False,\n",
       "  \"contains(('flight', 'tomorrow'))\": False,\n",
       "  \"contains(('please', 'help'))\": False,\n",
       "  \"contains(('thanks', 'JetBlue'))\": False,\n",
       "  \"contains(('AmericanAir', 'yes'))\": False,\n",
       "  \"contains(('Please', 'help'))\": False,\n",
       "  \"contains(('b', 'c'))\": False,\n",
       "  \"contains(('good', 'work'))\": False,\n",
       "  \"contains(('help', 'SouthwestAir'))\": False,\n",
       "  \"contains(('need', 'help'))\": False,\n",
       "  \"contains(('Looking', 'forward'))\": False,\n",
       "  \"contains(('could', 'get'))\": False,\n",
       "  \"contains(('guys', 'rock'))\": False,\n",
       "  \"contains(('still', 'waiting'))\": False,\n",
       "  \"contains(('JetBlue', 'flight'))\": False,\n",
       "  \"contains(('best', 'airline'))\": False,\n",
       "  \"contains(('flight', 'JetBlue'))\": False,\n",
       "  \"contains(('united', 'You'))\": False,\n",
       "  \"contains(('Cancelled', 'Flightlation'))\": False,\n",
       "  \"contains(('Wall', 'Street'))\": False,\n",
       "  \"contains(('confirmation', 'number'))\": False,\n",
       "  \"contains(('even', 'though'))\": False,\n",
       "  \"contains(('make', 'sure'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Just'))\": False,\n",
       "  \"contains(('one', 'way'))\": False,\n",
       "  \"contains(('service', 'united'))\": False,\n",
       "  \"contains(('JetBlue', 'great'))\": False,\n",
       "  \"contains(('Journal', 'http'))\": False,\n",
       "  \"contains(('Just', 'sent'))\": False,\n",
       "  \"contains(('email', 'address'))\": False,\n",
       "  \"contains(('flight', 'today'))\": False,\n",
       "  \"contains(('get', 'back'))\": False,\n",
       "  \"contains(('help', 'USAirways'))\": False,\n",
       "  \"contains(('service', 'SouthwestAir'))\": False,\n",
       "  \"contains(('united', 'Hi'))\": False,\n",
       "  \"contains(('Any', 'way'))\": False,\n",
       "  \"contains(('SouthwestAir', 'yes'))\": False,\n",
       "  \"contains(('great', 'job'))\": False,\n",
       "  \"contains(('time', 'flying'))\": False,\n",
       "  \"contains(('Flightled', 'flight'))\": False,\n",
       "  \"contains(('Thank', 'You'))\": False,\n",
       "  \"contains(('direct', 'flight'))\": False,\n",
       "  \"contains(('got', 'us'))\": False,\n",
       "  \"contains(('next', 'day'))\": False,\n",
       "  \"contains(('service', 'today'))\": False,\n",
       "  \"contains(('worst', 'customer'))\": False,\n",
       "  \"contains(('Great', 'job'))\": False,\n",
       "  \"contains(('SouthwestAir', 'love_dragonss'))\": False,\n",
       "  \"contains(('USAirways', 'flight'))\": False,\n",
       "  \"contains(('great', 'customer'))\": False,\n",
       "  \"contains(('help', 'JetBlue'))\": False,\n",
       "  \"contains(('service', 'AmericanAir'))\": False,\n",
       "  \"contains(('Any', 'chance'))\": False,\n",
       "  \"contains(('Looks', 'like'))\": False,\n",
       "  \"contains(('USAirways', 'Thanks'))\": False,\n",
       "  \"contains(('flight', 'AmericanAir'))\": False,\n",
       "  \"contains(('gate', 'agents'))\": False,\n",
       "  \"contains(('get', 'us'))\": False,\n",
       "  \"contains(('let', 'us'))\": False,\n",
       "  \"contains(('phone', 'number'))\": False,\n",
       "  \"contains(('social', 'media'))\": False,\n",
       "  \"contains(('taking', 'care'))\": False,\n",
       "  \"contains(('today', 'SouthwestAir'))\": False,\n",
       "  \"contains(('united', 'What'))\": False,\n",
       "  \"contains(('AmericanAir', 'USAirways'))\": False,\n",
       "  \"contains(('Great', 'service'))\": False,\n",
       "  \"contains(('SouthwestAir', 'please'))\": False,\n",
       "  \"contains(('VirginAmerica', 'Thanks'))\": False,\n",
       "  \"contains(('bad', 'weather'))\": False,\n",
       "  \"contains(('help', 'us'))\": False,\n",
       "  \"contains(('last', 'week'))\": False,\n",
       "  \"contains(('SouthwestAir', 'FortuneMagazine'))\": False,\n",
       "  \"contains(('every', 'time'))\": False,\n",
       "  \"contains(('flight', 'USAirways'))\": False,\n",
       "  \"contains(('flight', 'got'))\": False,\n",
       "  \"contains(('get', 'tickets'))\": False,\n",
       "  \"contains(('home', 'united'))\": False,\n",
       "  \"contains(('reLate', 'Flightd'))\": False,\n",
       "  \"contains(('round', 'trip'))\": False,\n",
       "  \"contains(('service', 'USAirways'))\": False,\n",
       "  \"contains(('service', 'rep'))\": False,\n",
       "  \"contains(('today', 'united'))\": False,\n",
       "  \"contains(('united', 'please'))\": False,\n",
       "  \"contains(('Customer', 'Service'))\": False,\n",
       "  \"contains(('San', 'Diego'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Hi'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Imaginedragons'))\": False,\n",
       "  \"contains(('USAirways', 'please'))\": False,\n",
       "  \"contains(('USAirways', 'yes'))\": False,\n",
       "  \"contains(('back', 'home'))\": False,\n",
       "  \"contains(('delayed', 'flight'))\": False,\n",
       "  \"contains(('getting', 'us'))\": False,\n",
       "  \"contains(('great', 'service'))\": False,\n",
       "  \"contains(('please', 'follow'))\": False,\n",
       "  \"contains(('see', 'Imaginedragons'))\": False,\n",
       "  \"contains(('united', 'airlines'))\": False,\n",
       "  \"contains(('American', 'Airlines'))\": False,\n",
       "  \"contains(('AmericanAir', 'flight'))\": False,\n",
       "  \"contains(('How', 'long'))\": False,\n",
       "  \"contains(('No', 'one'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Flight'))\": False,\n",
       "  \"contains(('VirginAmerica', 'thanks'))\": False,\n",
       "  \"contains(('favorite', 'airline'))\": False,\n",
       "  \"contains(('flights', 'AmericanAir'))\": False,\n",
       "  \"contains(('love', 'flying'))\": False,\n",
       "  \"contains(('never', 'fly'))\": False,\n",
       "  \"contains(('next', 'time'))\": False,\n",
       "  \"contains(('two', 'hours'))\": False,\n",
       "  \"contains(('united', 'Can'))\": False,\n",
       "  \"contains(('worst', 'airline'))\": False,\n",
       "  \"contains(('Flightled', 'flights'))\": False,\n",
       "  \"contains(('JetBlue', 'yes'))\": False,\n",
       "  \"contains(('another', 'airline'))\": False,\n",
       "  \"contains(('change', 'fee'))\": False,\n",
       "  \"contains(('credit', 'card'))\": False,\n",
       "  \"contains(('flight', 'yesterday'))\": False,\n",
       "  \"contains(('got', 'Cancelled'))\": False,\n",
       "  \"contains(('tomorrow', 'JetBlue'))\": False,\n",
       "  \"contains(('two', 'days'))\": False,\n",
       "  \"contains(('united', 'Yes'))\": False,\n",
       "  \"contains(('us', 'know'))\": False,\n",
       "  \"contains(('Cancelled', 'Flights'))\": False,\n",
       "  \"contains(('Flight', 'flight'))\": False,\n",
       "  \"contains(('My', 'flight'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Are'))\": False,\n",
       "  \"contains(('SouthwestAir', 'great'))\": False,\n",
       "  \"contains(('USAirways', 'Cancelled'))\": False,\n",
       "  \"contains(('VirginAmerica', 'You'))\": False,\n",
       "  \"contains(('airline', 'SouthwestAir'))\": False,\n",
       "  \"contains(('amazing', 'customer'))\": False,\n",
       "  \"contains(('appease', 'passengers'))\": False,\n",
       "  \"contains(('boarding', 'passes'))\": False,\n",
       "  \"contains(('companion', 'pass'))\": False,\n",
       "  \"contains(('flight', 'change'))\": False,\n",
       "  \"contains(('flight', 'home'))\": False,\n",
       "  \"contains(('please', 'passengers'))\": False,\n",
       "  \"contains(('return', 'flight'))\": False,\n",
       "  \"contains(('right', 'balance'))\": False,\n",
       "  \"contains(('united', 'How'))\": False,\n",
       "  \"contains(('AmericanAir', 'My'))\": False,\n",
       "  \"contains(('Any', 'idea'))\": False,\n",
       "  \"contains(('Flight', 'Cancelled'))\": False,\n",
       "  \"contains(('JetBlue', 'Flight'))\": False,\n",
       "  \"contains(('Much', 'appreciated'))\": False,\n",
       "  \"contains(('No', 'worries'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Any'))\": False,\n",
       "  \"contains(('SouthwestAir', 'How'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Is'))\": False,\n",
       "  \"contains(('call', 'center'))\": False,\n",
       "  \"contains(('keeping', 'us'))\": False,\n",
       "  \"contains(('united', 'Just'))\": False,\n",
       "  \"contains(('AmericanAir', 'Can'))\": False,\n",
       "  \"contains(('AmericanAir', 'great'))\": False,\n",
       "  \"contains(('AmericanAir', 'please'))\": False,\n",
       "  \"contains(('JetBlue', 'ok'))\": False,\n",
       "  \"contains(('On', 'hold'))\": False,\n",
       "  \"contains(('SouthwestAir', 'flight'))\": False,\n",
       "  \"contains(('SouthwestAir', 'love'))\": False,\n",
       "  \"contains(('USAirways', 'Can'))\": False,\n",
       "  \"contains(('USAirways', 'No'))\": False,\n",
       "  \"contains(('awesome', 'flight'))\": False,\n",
       "  \"contains(('cust', 'service'))\": False,\n",
       "  \"contains(('customer', 'relations'))\": False,\n",
       "  \"contains(('follow', 'back'))\": False,\n",
       "  \"contains(('last', 'flight'))\": False,\n",
       "  \"contains(('look', 'forward'))\": False,\n",
       "  \"contains(('mechanical', 'issues'))\": False,\n",
       "  \"contains(('much', 'better'))\": False,\n",
       "  \"contains(('people', 'working'))\": False,\n",
       "  \"contains(('service', 'desk'))\": False,\n",
       "  \"contains(('taken', 'care'))\": False,\n",
       "  \"contains(('united', 'My'))\": False,\n",
       "  \"contains(('united', 'ok'))\": False,\n",
       "  \"contains(('Any', 'help'))\": False,\n",
       "  \"contains(('Fingers', 'crossed'))\": False,\n",
       "  \"contains(('Imaginedragons', 'show'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Can'))\": False,\n",
       "  \"contains(('SouthwestAir', 'sent'))\": False,\n",
       "  \"contains(('come', 'back'))\": False,\n",
       "  \"contains(('days', 'ago'))\": False,\n",
       "  \"contains(('direct', 'flights'))\": False,\n",
       "  \"contains(('finally', 'got'))\": False,\n",
       "  \"contains(('flight', 'http'))\": False,\n",
       "  \"contains(('flights', 'SouthwestAir'))\": False,\n",
       "  \"contains(('free', 'wifi'))\": False,\n",
       "  \"contains(('ground', 'crew'))\": False,\n",
       "  \"contains(('last', 'time'))\": False,\n",
       "  \"contains(('ok', 'thank'))\": False,\n",
       "  \"contains(('service', 'JetBlue'))\": False,\n",
       "  \"contains(('time', 'SouthwestAir'))\": False,\n",
       "  \"contains(('tomorrow', 'united'))\": False,\n",
       "  \"contains(('tonight', 'united'))\": False,\n",
       "  \"contains(('u', 'guys'))\": False,\n",
       "  \"contains(('year', 'old'))\": False,\n",
       "  \"contains(('yes', 'please'))\": False,\n",
       "  \"contains(('American', 'http'))\": False,\n",
       "  \"contains(('At', 'least'))\": False,\n",
       "  \"contains(('Booking', 'Problemss'))\": False,\n",
       "  \"contains(('JetBlue', 'We'))\": False,\n",
       "  \"contains(('Las', 'Vegas'))\": False,\n",
       "  \"contains(('VirginAmerica', 'ladygaga'))\": False,\n",
       "  \"contains(('airline', 'united'))\": False,\n",
       "  \"contains(('big', 'thanks'))\": False,\n",
       "  \"contains(('first', 'flight'))\": False,\n",
       "  \"contains(('flights', 'going'))\": False,\n",
       "  \"contains(('give', 'us'))\": False,\n",
       "  \"contains(('great', 'united'))\": False,\n",
       "  \"contains(('home', 'today'))\": False,\n",
       "  \"contains(('hour', 'wait'))\": False,\n",
       "  \"contains(('hours', 'ago'))\": False,\n",
       "  \"contains(('ladygaga', 'carrieunderwood'))\": False,\n",
       "  \"contains(('last', 'year'))\": False,\n",
       "  \"contains(('months', 'ago'))\": False,\n",
       "  \"contains(('much', 'appreciated'))\": False,\n",
       "  \"contains(('phone', 'call'))\": False,\n",
       "  \"contains(('please', 'JetBlue'))\": False,\n",
       "  \"contains(('please', 'united'))\": False,\n",
       "  \"contains(('service', 'Thank'))\": False,\n",
       "  \"contains(('today', 'AmericanAir'))\": False,\n",
       "  \"contains(('united', 'If'))\": False,\n",
       "  \"contains(('united', 'Why'))\": False,\n",
       "  \"contains(('united', 'flt'))\": False,\n",
       "  \"contains(('united', 'well'))\": False,\n",
       "  \"contains(('weeks', 'ago'))\": False,\n",
       "  \"contains(('Airport', 'snow'))\": False,\n",
       "  \"contains(('AmericanAir', 'Hi'))\": False,\n",
       "  \"contains(('AmericanAir', 'got'))\": False,\n",
       "  \"contains(('JetBlue', 'oh'))\": False,\n",
       "  \"contains(('JetBlue', 'well'))\": False,\n",
       "  \"contains(('Just', 'got'))\": False,\n",
       "  \"contains(('Please', 'follow'))\": False,\n",
       "  \"contains(('San', 'Juan'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Thx'))\": False,\n",
       "  \"contains(('SouthwestAir', 'When'))\": False,\n",
       "  \"contains(('SouthwestAir', 'hey'))\": False,\n",
       "  \"contains(('USAirways', 'ok'))\": False,\n",
       "  \"contains(('We', 'need'))\": False,\n",
       "  \"contains(('airline', 'ever'))\": False,\n",
       "  \"contains(('baggage', 'fees'))\": False,\n",
       "  \"contains(('customer', 'care'))\": False,\n",
       "  \"contains(('excellent', 'service'))\": False,\n",
       "  \"contains(('flights', 'united'))\": False,\n",
       "  \"contains(('go', 'back'))\": False,\n",
       "  \"contains(('non', 'stop'))\": False,\n",
       "  \"contains(('ok', 'thanks'))\": False,\n",
       "  \"contains(('please', 'give'))\": False,\n",
       "  \"contains(('removal', 'method'))\": False,\n",
       "  \"contains(('service', 'ever'))\": False,\n",
       "  \"contains(('snow', 'removal'))\": False,\n",
       "  \"contains(('tomorrow', 'SouthwestAir'))\": False,\n",
       "  \"contains(('tomorrow', 'USAirways'))\": False,\n",
       "  \"contains(('united', 'The'))\": False,\n",
       "  \"contains(('us', 'home'))\": False,\n",
       "  \"contains(('weather', 'reLate'))\": False,\n",
       "  \"contains(('work', 'folks'))\": False,\n",
       "  \"contains(('would', 'go'))\": False,\n",
       "  \"contains(('would', 'make'))\": False,\n",
       "  \"contains(('1st', 'time'))\": False,\n",
       "  \"contains(('Customer', 'service'))\": False,\n",
       "  \"contains(('Flight', 'united'))\": False,\n",
       "  \"contains(('Flighted', 'flight'))\": False,\n",
       "  \"contains(('In', 'Flight'))\": False,\n",
       "  \"contains(('JetBlue', 'Just'))\": False,\n",
       "  \"contains(('Reminder', 'From'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Great'))\": False,\n",
       "  \"contains(('SouthwestAir', 'So'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Southwest'))\": False,\n",
       "  \"contains(('SouthwestAir', 'The'))\": False,\n",
       "  \"contains(('Thank', 'u'))\": False,\n",
       "  \"contains(('USAirways', 'Reminder'))\": False,\n",
       "  \"contains(('USAirways', 'Yes'))\": False,\n",
       "  \"contains(('United', 'flight'))\": False,\n",
       "  \"contains(('change', 'flight'))\": False,\n",
       "  \"contains(('day', 'AmericanAir'))\": False,\n",
       "  \"contains(('delayed', 'due'))\": False,\n",
       "  \"contains(('domestic', 'flight'))\": False,\n",
       "  \"contains(('great', 'crew'))\": False,\n",
       "  \"contains(('great', 'trip'))\": False,\n",
       "  \"contains(('hours', 'SouthwestAir'))\": False,\n",
       "  \"contains(('hrs', 'Late'))\": False,\n",
       "  \"contains(('jet', 'blue'))\": False,\n",
       "  \"contains(('last', 'month'))\": False,\n",
       "  \"contains(('looking', 'forward'))\": False,\n",
       "  \"contains(('much', 'AmericanAir'))\": False,\n",
       "  \"contains(('much', 'united'))\": False,\n",
       "  \"contains(('new', 'plane'))\": False,\n",
       "  \"contains(('origin', 'destination'))\": False,\n",
       "  \"contains(('please', 'AmericanAir'))\": False,\n",
       "  \"contains(('really', 'appreciate'))\": False,\n",
       "  \"contains(('rental', 'car'))\": False,\n",
       "  \"contains(('response', 'SouthwestAir'))\": False,\n",
       "  \"contains(('response', 'united'))\": False,\n",
       "  \"contains(('u', 'help'))\": False,\n",
       "  \"contains(('united', 'Do'))\": False,\n",
       "  \"contains(('united', 'It'))\": False,\n",
       "  \"contains(('united', 'We'))\": False,\n",
       "  \"contains(('united', 'Will'))\": False,\n",
       "  \"contains(('us', 'http'))\": False,\n",
       "  \"contains(('via', 'Twitter'))\": False,\n",
       "  \"contains(('weather', 'delay'))\": False,\n",
       "  \"contains(('Airways', 'Corporation'))\": False,\n",
       "  \"contains(('AmericanAir', 'It'))\": False,\n",
       "  \"contains(('AmericanAir', 'dfwairport'))\": False,\n",
       "  \"contains(('Another', 'reason'))\": False,\n",
       "  \"contains(('CheapFlights', 'FareCompare'))\": False,\n",
       "  \"contains(('Daily', 'Journal'))\": False,\n",
       "  \"contains(('DestinationDragons', 'http'))\": False,\n",
       "  \"contains(('Hi', 'guys'))\": False,\n",
       "  \"contains(('It', 'would'))\": False,\n",
       "  \"contains(('JetBlue', 'JayVig'))\": False,\n",
       "  \"contains(('Republican', 'American'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Awesome'))\": False,\n",
       "  \"contains(('SouthwestAir', 'What'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Your'))\": False,\n",
       "  \"contains(('SouthwestAir', 'got'))\": False,\n",
       "  \"contains(('Street', 'Waterbury'))\": False,\n",
       "  \"contains(('Thanks', 'It'))\": False,\n",
       "  \"contains(('Thanks', 'VirginAmerica'))\": False,\n",
       "  \"contains(('USAirways', 'Flight'))\": False,\n",
       "  \"contains(('Wall', 'Daily'))\": False,\n",
       "  \"contains(('Waterbury', 'Republican'))\": False,\n",
       "  \"contains(('another', 'airport'))\": False,\n",
       "  \"contains(('appreciated', 'USAirways'))\": False,\n",
       "  \"contains(('back', 'Late'))\": False,\n",
       "  \"contains(('best', 'customer'))\": False,\n",
       "  \"contains(('best', 'flight'))\": False,\n",
       "  \"contains(('cities', 'http'))\": False,\n",
       "  \"contains(('cool', 'cities'))\": False,\n",
       "  \"contains(('day', 'united'))\": False,\n",
       "  \"contains(('delayed', 'flights'))\": False,\n",
       "  \"contains(('flight', 'How'))\": False,\n",
       "  \"contains(('flight', 'w'))\": False,\n",
       "  \"contains(('getaway', 'deals'))\": False,\n",
       "  \"contains(('got', 'rebooked'))\": False,\n",
       "  \"contains(('great', 'SouthwestAir'))\": False,\n",
       "  \"contains(('great', 'people'))\": False,\n",
       "  \"contains(('high', 'five'))\": False,\n",
       "  \"contains(('hour', 'layover'))\": False,\n",
       "  \"contains(('last', 'minute'))\": False,\n",
       "  \"contains(('lost', 'bag'))\": False,\n",
       "  \"contains(('middle', 'seat'))\": False,\n",
       "  \"contains(('new', 'flight'))\": False,\n",
       "  \"contains(('next', 'week'))\": False,\n",
       "  \"contains(('reservation', 'online'))\": False,\n",
       "  \"contains(('service', 'line'))\": False,\n",
       "  \"contains(('someone', 'please'))\": False,\n",
       "  \"contains(('super', 'helpful'))\": False,\n",
       "  \"contains(('time', 'united'))\": False,\n",
       "  \"contains(('united', 'thnx'))\": False,\n",
       "  \"contains(('us', 'back'))\": False,\n",
       "  \"contains(('way', 'Lots'))\": False,\n",
       "  \"contains(('AmericanAir', 'No'))\": False,\n",
       "  \"contains(('AmericanAir', 'SouthwestAir'))\": False,\n",
       "  \"contains(('AmericanAir', 'We'))\": False,\n",
       "  \"contains(('AmericanAir', 'trying'))\": False,\n",
       "  \"contains(('Can', 'u'))\": False,\n",
       "  \"contains(('Cancelled', 'Flightling'))\": False,\n",
       "  \"contains(('Digital', 'Journal'))\": False,\n",
       "  \"contains(('First', 'Class'))\": False,\n",
       "  \"contains(('Imagine', 'Dragons'))\": False,\n",
       "  \"contains(('Jet', 'Blue'))\": False,\n",
       "  \"contains(('Not', 'cool'))\": False,\n",
       "  \"contains(('Palm', 'Springs'))\": False,\n",
       "  \"contains(('Southwest', 'http'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Hey'))\": False,\n",
       "  \"contains(('Thanks', 'David'))\": False,\n",
       "  \"contains(('That', 'would'))\": False,\n",
       "  \"contains(('Thx', 'united'))\": False,\n",
       "  \"contains(('USAirways', 'How'))\": False,\n",
       "  \"contains(('USAirways', 'My'))\": False,\n",
       "  \"contains(('Vegas', 'event'))\": False,\n",
       "  \"contains(('With', 'New'))\": False,\n",
       "  \"contains(('Your', 'customer'))\": False,\n",
       "  \"contains(('airline', 'AmericanAir'))\": False,\n",
       "  \"contains(('appreciated', 'united'))\": False,\n",
       "  \"contains(('call', 'volume'))\": False,\n",
       "  \"contains(('car', 'seat'))\": False,\n",
       "  \"contains(('co', '8WBzOrRn3C'))\": False,\n",
       "  \"contains(('co', 'rfXlV1kGDh'))\": False,\n",
       "  \"contains(('could', 'use'))\": False,\n",
       "  \"contains(('day', 'SouthwestAir'))\": False,\n",
       "  \"contains(('flight', 'time'))\": False,\n",
       "  \"contains(('great', 'JetBlue'))\": False,\n",
       "  \"contains(('great', 'thanks'))\": False,\n",
       "  \"contains(('guys', 'USAirways'))\": False,\n",
       "  \"contains(('keep', 'getting'))\": False,\n",
       "  \"contains(('media', 'team'))\": False,\n",
       "  \"contains(('morning', 'SouthwestAir'))\": False,\n",
       "  \"contains(('much', 'SouthwestAir'))\": False,\n",
       "  \"contains(('number', 'united'))\": False,\n",
       "  \"contains(('please', 'get'))\": False,\n",
       "  \"contains(('please', 'please'))\": False,\n",
       "  \"contains(('priority', 'boarding'))\": False,\n",
       "  \"contains(('prompt', 'response'))\": False,\n",
       "  \"contains(('red', 'eye'))\": False,\n",
       "  \"contains(('response', 'USAirways'))\": False,\n",
       "  \"contains(('start', 'flying'))\": False,\n",
       "  \"contains(('thank', 'u'))\": False,\n",
       "  \"contains(('united', 'Flight'))\": False,\n",
       "  \"contains(('united', 'So'))\": False,\n",
       "  \"contains(('united', 'awesome'))\": False,\n",
       "  \"contains(('us', 'airways'))\": False,\n",
       "  \"contains(('via', 'phone'))\": False,\n",
       "  \"contains(('wait', 'times'))\": False,\n",
       "  \"contains(('work', 'SouthwestAir'))\": False,\n",
       "  \"contains(('work', 'united'))\": False,\n",
       "  \"contains(('AmericanAir', 'Aww'))\": False,\n",
       "  \"contains(('Can', 'someone'))\": False,\n",
       "  \"contains(('Companion', 'Pass'))\": False,\n",
       "  \"contains(('Confirmation', 'number'))\": False,\n",
       "  \"contains(('DestinationDragons', 'AmericanAir'))\": False,\n",
       "  \"contains(('DestinationDragons', 'Imaginedragons'))\": False,\n",
       "  \"contains(('DestinationDragons', 'USAirways'))\": False,\n",
       "  \"contains(('Eastern', 'Airlines'))\": False,\n",
       "  \"contains(('Flight', 'Access'))\": False,\n",
       "  \"contains(('Flightled', 'united'))\": False,\n",
       "  \"contains(('Good', 'luck'))\": False,\n",
       "  \"contains(('Grandma', 'Ella'))\": False,\n",
       "  \"contains(('Is', 'flight'))\": False,\n",
       "  \"contains(('JetBlue', 'Hi'))\": False,\n",
       "  \"contains(('JetBlue', 'If'))\": False,\n",
       "  \"contains(('JetBlue', 'No'))\": False,\n",
       "  \"contains(('JetBlue', 'That'))\": False,\n",
       "  \"contains(('JetBlue', 'What'))\": False,\n",
       "  \"contains(('JetBlue', 'When'))\": False,\n",
       "  \"contains(('Love', 'Field'))\": False,\n",
       "  \"contains(('Need', 'help'))\": False,\n",
       "  \"contains(('New', 'York'))\": False,\n",
       "  \"contains(('News', 'http'))\": False,\n",
       "  \"contains(('Offer', 'In'))\": False,\n",
       "  \"contains(('San', 'Francisco'))\": False,\n",
       "  \"contains(('Short', 'Interest'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Flying'))\": False,\n",
       "  \"contains(('SouthwestAir', 'My'))\": False,\n",
       "  \"contains(('SouthwestAir', 'That'))\": False,\n",
       "  \"contains(('SouthwestAir', 'done'))\": False,\n",
       "  \"contains(('SouthwestAir', 'give'))\": False,\n",
       "  \"contains(('USAirways', 'You'))\": False,\n",
       "  \"contains(('Virgin', 'America'))\": False,\n",
       "  \"contains(('VirginAmerica', 'Is'))\": False,\n",
       "  \"contains(('airline', 'USAirways'))\": False,\n",
       "  \"contains(('airline', 'oscars2016'))\": False,\n",
       "  \"contains(('avgeek', 'http'))\": False,\n",
       "  \"contains(('crew', 'She'))\": False,\n",
       "  \"contains(('different', 'flight'))\": False,\n",
       "  \"contains(('early', 'frontrunner'))\": False,\n",
       "  \"contains(('early', 'morning'))\": False,\n",
       "  \"contains(('email', 'united'))\": False,\n",
       "  \"contains(('entire', 'flight'))\": False,\n",
       "  \"contains(('event', 'March'))\": False,\n",
       "  \"contains(('ever', 'experienced'))\": False,\n",
       "  \"contains(('excellent', 'customer'))\": False,\n",
       "  \"contains(('feel', 'like'))\": False,\n",
       "  \"contains(('flight', 'Thanks'))\": False,\n",
       "  \"contains(('flight', 'entertainment'))\": False,\n",
       "  \"contains(('flights', 'USAirways'))\": False,\n",
       "  \"contains(('gate', 'united'))\": False,\n",
       "  \"contains(('get', 'stuck'))\": False,\n",
       "  \"contains(('got', 'thru'))\": False,\n",
       "  \"contains(('guys', 'AmericanAir'))\": False,\n",
       "  \"contains(('guys', 'JetBlue'))\": False,\n",
       "  \"contains(('help', 'VirginAmerica'))\": False,\n",
       "  \"contains(('hold', 'times'))\": False,\n",
       "  \"contains(('home', 'AmericanAir'))\": False,\n",
       "  \"contains(('home', 'SouthwestAir'))\": False,\n",
       "  \"contains(('home', 'USAirways'))\": False,\n",
       "  \"contains(('join', 'us'))\": False,\n",
       "  \"contains(('kind', 'words'))\": False,\n",
       "  \"contains(('last', 'name'))\": False,\n",
       "  \"contains(('left', 'something'))\": False,\n",
       "  \"contains(('long', 'wait'))\": False,\n",
       "  \"contains(('look', 'like'))\": False,\n",
       "  \"contains(('missed', 'connection'))\": False,\n",
       "  \"contains(('plane', 'back'))\": False,\n",
       "  \"contains(('pleasantly', 'surprised'))\": False,\n",
       "  \"contains(('quick', 'reply'))\": False,\n",
       "  \"contains(('response', 'JetBlue'))\": False,\n",
       "  \"contains(('rock', 'AmericanAir'))\": False,\n",
       "  \"contains(('sit', 'together'))\": False,\n",
       "  \"contains(('someone', 'else'))\": False,\n",
       "  \"contains(('soon', 'united'))\": False,\n",
       "  \"contains(('time', 'JetBlue'))\": False,\n",
       "  \"contains(('time', 'Thanks'))\": False,\n",
       "  \"contains(('today', 'Thanks'))\": False,\n",
       "  \"contains(('today', 'USAirways'))\": False,\n",
       "  \"contains(('travel', 'plans'))\": False,\n",
       "  \"contains(('united', 'Is'))\": False,\n",
       "  \"contains(('year', 'http'))\": False,\n",
       "  \"contains(('Airbus', 'Wow'))\": False,\n",
       "  \"contains(('Airport', 'http'))\": False,\n",
       "  \"contains(('AmericanAir', 'Yes'))\": False,\n",
       "  \"contains(('Atlanta', 'show'))\": False,\n",
       "  \"contains(('Black', 'History'))\": False,\n",
       "  \"contains(('Bluemanity', 'CoreValues'))\": False,\n",
       "  \"contains(('CoreValues', 'Passion'))\": False,\n",
       "  \"contains(('Costa', 'Rica'))\": False,\n",
       "  \"contains(('DestinationDragons', 'united'))\": False,\n",
       "  \"contains(('Dividend', 'Miles'))\": False,\n",
       "  \"contains(('F2LFULCbQ7', 'let'))\": False,\n",
       "  \"contains(('How', 'many'))\": False,\n",
       "  \"contains(('Imaginedragons', 'DestinationDragons'))\": False,\n",
       "  \"contains(('JetBlue', 'Airbus'))\": False,\n",
       "  \"contains(('JetBlue', 'Great'))\": False,\n",
       "  \"contains(('JetBlue', 'USAirways'))\": False,\n",
       "  \"contains(('JetBlue', 'Well'))\": False,\n",
       "  \"contains(('JetBlue', 'hi'))\": False,\n",
       "  \"contains(('JetBlue', 'u'))\": False,\n",
       "  \"contains(('JetBlue', 'would'))\": False,\n",
       "  \"contains(('Love', 'flying'))\": False,\n",
       "  \"contains(('My', 'wife'))\": False,\n",
       "  \"contains(('New', 'marketing'))\": False,\n",
       "  \"contains(('Passion', 'AeroJobMarket'))\": False,\n",
       "  \"contains(('So', 'thanks'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Yes'))\": False,\n",
       "  \"contains(('USAirways', 'Do'))\": False,\n",
       "  \"contains(('USAirways', 'Your'))\": False,\n",
       "  \"contains(('Was', 'able'))\": False,\n",
       "  \"contains(('Your', 'crew'))\": False,\n",
       "  \"contains(('agent', 'said'))\": False,\n",
       "  \"contains(('amp', 'beyond'))\": False,\n",
       "  \"contains(('baggage', 'office'))\": False,\n",
       "  \"contains(('becoming', 'pilots'))\": False,\n",
       "  \"contains(('best', 'friend'))\": False,\n",
       "  \"contains(('book', 'online'))\": False,\n",
       "  \"contains(('business', 'class'))\": False,\n",
       "  \"contains(('co', 'F2LFULCbQ7'))\": False,\n",
       "  \"contains(('co', 'dbcvEPn5QC'))\": False,\n",
       "  \"contains(('cross', 'country'))\": False,\n",
       "  \"contains(('dbcvEPn5QC', 'Great'))\": False,\n",
       "  \"contains(('e', 'mail'))\": False,\n",
       "  \"contains(('en', 'route'))\": False,\n",
       "  \"contains(('flight', 'amp'))\": False,\n",
       "  \"contains(('flight', 'back'))\": False,\n",
       "  \"contains(('flight', 'delay'))\": False,\n",
       "  \"contains(('flight', 'ever'))\": False,\n",
       "  \"contains(('flight', 'leaves'))\": False,\n",
       "  \"contains(('flights', 'Cancelled'))\": False,\n",
       "  \"contains(('flying', 'Southwest'))\": False,\n",
       "  \"contains(('flying', 'experience'))\": False,\n",
       "  \"contains(('get', 'thru'))\": False,\n",
       "  \"contains(('go', 'home'))\": False,\n",
       "  \"contains(('good', 'day'))\": False,\n",
       "  \"contains(('hour', 'Late'))\": False,\n",
       "  \"contains(('hour', 'flight'))\": False,\n",
       "  \"contains(('hours', 'Thanks'))\": False,\n",
       "  \"contains(('international', 'flight'))\": False,\n",
       "  \"contains(('know', 'SouthwestAir'))\": False,\n",
       "  \"contains(('long', 'day'))\": False,\n",
       "  \"contains(('lt', 'lt'))\": False,\n",
       "  \"contains(('marketing', 'song'))\": False,\n",
       "  \"contains(('minutes', 'ago'))\": False,\n",
       "  \"contains(('morning', 'JetBlue'))\": False,\n",
       "  \"contains(('morning', 'flights'))\": False,\n",
       "  \"contains(('never', 'got'))\": False,\n",
       "  \"contains(('okay', 'thanks'))\": False,\n",
       "  \"contains(('please', 'SouthwestAir'))\": False,\n",
       "  \"contains(('please', 'Thanks'))\": False,\n",
       "  \"contains(('please', 'USAirways'))\": False,\n",
       "  \"contains(('please', 'fix'))\": False,\n",
       "  \"contains(('service', 'agent'))\": False,\n",
       "  \"contains(('several', 'times'))\": False,\n",
       "  \"contains(('snow', 'storm'))\": False,\n",
       "  \"contains(('song', 'https'))\": False,\n",
       "  \"contains(('still', 'shows'))\": False,\n",
       "  \"contains(('tell', 'us'))\": False,\n",
       "  \"contains(('ticket', 'agent'))\": False,\n",
       "  \"contains(('tomorrow', 'AmericanAir'))\": False,\n",
       "  \"contains(('took', 'care'))\": False,\n",
       "  \"contains(('trip', 'today'))\": False,\n",
       "  \"contains(('united', 'No'))\": False,\n",
       "  \"contains(('united', 'look'))\": False,\n",
       "  \"contains(('united', 'united'))\": False,\n",
       "  \"contains(('would', 'appreciate'))\": False,\n",
       "  \"contains(('would', 'rather'))\": False,\n",
       "  \"contains(('2nd', 'time'))\": False,\n",
       "  \"contains(('AmericanAir', 'Airport'))\": False,\n",
       "  \"contains(('AmericanAir', 'Great'))\": False,\n",
       "  \"contains(('AmericanAir', 'united'))\": False,\n",
       "  \"contains(('AmericanAir', 'well'))\": False,\n",
       "  \"contains(('Are', 'flights'))\": False,\n",
       "  \"contains(('DestinationDragons', 'JetBlue'))\": False,\n",
       "  \"contains(('DestinationDragons', 'SouthwestAir'))\": False,\n",
       "  \"contains(('Flight', 'flights'))\": False,\n",
       "  \"contains(('Flightled', 'Can'))\": False,\n",
       "  \"contains(('Flightled', 'USAirways'))\": False,\n",
       "  \"contains(('Flightled', 'tomorrow'))\": False,\n",
       "  \"contains(('Great', 'customer'))\": False,\n",
       "  \"contains(('Help', 'SouthwestAir'))\": False,\n",
       "  \"contains(('Help', 'united'))\": False,\n",
       "  \"contains(('JetBlue', 'Hey'))\": False,\n",
       "  \"contains(('JetBlue', 'There'))\": False,\n",
       "  \"contains(('Just', 'wanted'))\": False,\n",
       "  \"contains(('Never', 'flying'))\": False,\n",
       "  \"contains(('Never', 'got'))\": False,\n",
       "  \"contains(('Nice', 'job'))\": False,\n",
       "  \"contains(('Not', 'impressed'))\": False,\n",
       "  \"contains(('SouthwestAir', 'All'))\": False,\n",
       "  \"contains(('SouthwestAir', 'DestinationDragons'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Do'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Had'))\": False,\n",
       "  \"contains(('SouthwestAir', 'flying'))\": False,\n",
       "  \"contains(('SouthwestAir', 'think'))\": False,\n",
       "  \"contains(('SouthwestAir', 'thx'))\": False,\n",
       "  \"contains(('SouthwestAir', 'would'))\": False,\n",
       "  \"contains(('Thx', 'SouthwestAir'))\": False,\n",
       "  \"contains(('USAirways', 'Hi'))\": False,\n",
       "  \"contains(('USAirways', 'This'))\": False,\n",
       "  \"contains(('USAirways', 'We'))\": False,\n",
       "  \"contains(('USAirways', 'Will'))\": False,\n",
       "  \"contains(('United', 'Club'))\": False,\n",
       "  \"contains(('VirginAmerica', 'Are'))\": False,\n",
       "  \"contains(('VirginAmerica', 'come'))\": False,\n",
       "  \"contains(('account', 'SouthwestAir'))\": False,\n",
       "  \"contains(('amp', 'crew'))\": False,\n",
       "  \"contains(('amp', 'got'))\": False,\n",
       "  \"contains(('anyone', 'help'))\": False,\n",
       "  \"contains(('appreciated', 'SouthwestAir'))\": False,\n",
       "  \"contains(('award', 'travel'))\": False,\n",
       "  \"contains(('back', 'tomorrow'))\": False,\n",
       "  \"contains(('boarding', 'time'))\": False,\n",
       "  \"contains(('cabin', 'crew'))\": False,\n",
       "  \"contains(('change', 'fees'))\": False,\n",
       "  \"contains(('checked', 'bags'))\": False,\n",
       "  \"contains(('co', 'oUmC1LrXDN'))\": False,\n",
       "  \"contains(('coming', 'Thanks'))\": False,\n",
       "  \"contains(('cool', 'JetBlue'))\": False,\n",
       "  \"contains(('day', 'trip'))\": False,\n",
       "  \"contains(('days', 'united'))\": False,\n",
       "  \"contains(('delay', 'due'))\": False,\n",
       "  \"contains(('departure', 'time'))\": False,\n",
       "  \"contains(('dividend', 'miles'))\": False,\n",
       "  \"contains(('done', 'SouthwestAir'))\": False,\n",
       "  \"contains(('experience', 'ever'))\": False,\n",
       "  \"contains(('flat', 'seating'))\": False,\n",
       "  \"contains(('flight', 'Thank'))\": False,\n",
       "  \"contains(('flight', 'delayed'))\": False,\n",
       "  \"contains(('flight', 'number'))\": False,\n",
       "  \"contains(('flight', 'online'))\": False,\n",
       "  \"contains(('flight', 'without'))\": False,\n",
       "  \"contains(('full', 'day'))\": False,\n",
       "  \"contains(('golf', 'bag'))\": False,\n",
       "  \"contains(('great', 'experience'))\": False,\n",
       "  \"contains(('ground', 'staff'))\": False,\n",
       "  \"contains(('guy', 'named'))\": False,\n",
       "  \"contains(('happy', 'customer'))\": False,\n",
       "  \"contains(('horrible', 'customer'))\": False,\n",
       "  \"contains(('hour', 'early'))\": False,\n",
       "  \"contains(('hours', 'united'))\": False,\n",
       "  \"contains(('huge', 'fan'))\": False,\n",
       "  \"contains(('job', 'united'))\": False,\n",
       "  \"contains(('keep', 'checking'))\": False,\n",
       "  \"contains(('lie', 'flat'))\": False,\n",
       "  \"contains(('life', 'SouthwestAir'))\": False,\n",
       "  \"contains(('long', 'time'))\": False,\n",
       "  \"contains(('lost', 'item'))\": False,\n",
       "  \"contains(('lovely', 'flight'))\": False,\n",
       "  \"contains(('many', 'times'))\": False,\n",
       "  \"contains(('mechanical', 'problems'))\": False,\n",
       "  \"contains(('mileage', 'plus'))\": False,\n",
       "  \"contains(('morning', 'Thanks'))\": False,\n",
       "  \"contains(('much', 'USAirways'))\": False,\n",
       "  \"contains(('new', 'routes'))\": False,\n",
       "  \"contains(('one', 'united'))\": False,\n",
       "  \"contains(('original', 'flight'))\": False,\n",
       "  \"contains(('past', 'week'))\": False,\n",
       "  \"contains(('phone', 'line'))\": False,\n",
       "  \"contains(('photo', 'http'))\": False,\n",
       "  \"contains(('plane', 'SouthwestAir'))\": False,\n",
       "  \"contains(('power', 'outlets'))\": False,\n",
       "  \"contains(('promo', 'code'))\": False,\n",
       "  \"contains(('real', 'person'))\": False,\n",
       "  \"contains(('record', 'locator'))\": False,\n",
       "  \"contains(('school', 'trip'))\": False,\n",
       "  \"contains(('seat', 'assignment'))\": False,\n",
       "  \"contains(('second', 'flight'))\": False,\n",
       "  \"contains(('sent', 'Thanks'))\": False,\n",
       "  \"contains(('sent', 'USAirways'))\": False,\n",
       "  \"contains(('status', 'match'))\": False,\n",
       "  \"contains(('time', 'AmericanAir'))\": False,\n",
       "  \"contains(('time', 'USAirways'))\": False,\n",
       "  \"contains(('today', 'JetBlue'))\": False,\n",
       "  \"contains(('tomorrow', 'amp'))\": False,\n",
       "  \"contains(('trip', 'united'))\": False,\n",
       "  \"contains(('two', 'different'))\": False,\n",
       "  \"contains(('u', 'r'))\": False,\n",
       "  \"contains(('united', 'Please'))\": False,\n",
       "  \"contains(('united', 'done'))\": False,\n",
       "  \"contains(('united', 'follow'))\": False,\n",
       "  \"contains(('united', 'great'))\": False,\n",
       "  \"contains(('united', 'sent'))\": False,\n",
       "  \"contains(('united', 'still'))\": False,\n",
       "  \"contains(('ur', 'planes'))\": False,\n",
       "  \"contains(('us', 'air'))\": False,\n",
       "  \"contains(('way', 'united'))\": False,\n",
       "  \"contains(('weather', 'united'))\": False,\n",
       "  \"contains(('would', 'get'))\": False,\n",
       "  \"contains(('yes', 'yes'))\": False,\n",
       "  \"contains(('Airways', 'connecting'))\": False,\n",
       "  \"contains(('AmericanAir', 'Do'))\": False,\n",
       "  \"contains(('AmericanAir', 'Is'))\": False,\n",
       "  \"contains(('AmericanAir', 'These'))\": False,\n",
       "  \"contains(('AmericanAir', 'This'))\": False,\n",
       "  \"contains(('AmericanAir', 'Your'))\": False,\n",
       "  \"contains(('AmericanAir', 'ok'))\": False,\n",
       "  \"contains(('AmericanAir', 'origin'))\": False,\n",
       "  \"contains(('Australia', 'http'))\": False,\n",
       "  \"contains(('Awesome', 'thanks'))\": False,\n",
       "  \"contains(('Big', 'thanks'))\": False,\n",
       "  \"contains(('Cancelled', 'Flightation'))\": False,\n",
       "  \"contains(('Cancelled', 'Flightations'))\": False,\n",
       "  \"contains(('Capt', 'Herman'))\": False,\n",
       "  \"contains(('Daily', 'http'))\": False,\n",
       "  \"contains(('Dragon', 'book'))\": False,\n",
       "  \"contains(('Flightled', 'http'))\": False,\n",
       "  \"contains(('Flightled', 'today'))\": False,\n",
       "  \"contains(('FlyTPA', 'following'))\": False,\n",
       "  \"contains(('Fort', 'Lauderdale'))\": False,\n",
       "  \"contains(('Have', 'fun'))\": False,\n",
       "  \"contains(('Huge', 'thanks'))\": False,\n",
       "  \"contains(('It', 'looks'))\": False,\n",
       "  \"contains(('JetBlue', 'Awesome'))\": False,\n",
       "  \"contains(('JetBlue', 'Haha'))\": False,\n",
       "  \"contains(('JetBlue', 'The'))\": False,\n",
       "  \"contains(('JetBlue', 'good'))\": False,\n",
       "  \"contains(('JetBlue', 'hey'))\": False,\n",
       "  \"contains(('JetBlue', 'loving'))\": False,\n",
       "  \"contains(('Keeping', 'traditions'))\": False,\n",
       "  \"contains(('Late', 'Flightly'))\": False,\n",
       "  \"contains(('Many', 'thanks'))\": False,\n",
       "  \"contains(('No', 'help'))\": False,\n",
       "  \"contains(('Opal', 'Dragon'))\": False,\n",
       "  \"contains(('San', 'Antonio'))\": False,\n",
       "  \"contains(('Sat', 'morning'))\": False,\n",
       "  \"contains(('So', 'excited'))\": False,\n",
       "  \"contains(('So', 'far'))\": False,\n",
       "  \"contains(('Southwest', 'You'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Love'))\": False,\n",
       "  \"contains(('SouthwestAir', 'Please'))\": False,\n",
       "  \"contains(('SouthwestAir', 'You'))\": False,\n",
       "  \"contains(('SouthwestAir', 'oh'))\": False,\n",
       "  \"contains(('Street', 'http'))\": False,\n",
       "  \"contains(('Thanks', 'You'))\": False,\n",
       "  \"contains(('The', 'Dragon'))\": False,\n",
       "  \"contains(('The', 'Opal'))\": False,\n",
       "  \"contains(('USAirways', 'It'))\": False,\n",
       "  \"contains(('USAirways', 'awesome'))\": False,\n",
       "  \"contains(('USAirways', 'sitting'))\": False,\n",
       "  \"contains(('USAirways', 'still'))\": False,\n",
       "  \"contains(('USAirways', 'tell'))\": False,\n",
       "  \"contains(('USAirways', 'trying'))\": False,\n",
       "  \"contains(('USAirways', 'worst'))\": False,\n",
       "  \"contains(('USAirways', 'yeah'))\": False,\n",
       "  \"contains(('United', 'Airlines'))\": False,\n",
       "  \"contains(('VirginAmerica', 'got'))\": False,\n",
       "  \"contains(('VirginAmerica', 'love'))\": False,\n",
       "  \"contains(('VirginAmerica', 'thank'))\": False,\n",
       "  \"contains(('What', 'gives'))\": False,\n",
       "  \"contains(('Wonderful', 'ambassador'))\": False,\n",
       "  \"contains(('airline', 'JetBlue'))\": False,\n",
       "  \"contains(('airline', 'around'))\": False,\n",
       "  \"contains(('airport', 'united'))\": False,\n",
       "  \"contains(('amazing', 'Came'))\": False,\n",
       "  \"contains(('amazing', 'JetBlue'))\": False,\n",
       "  \"contains(('amazing', 'airline'))\": False,\n",
       "  \"contains(('another', 'plane'))\": False,\n",
       "  \"contains(('anything', 'Wonderful'))\": False,\n",
       "  \"contains(('appreciated', 'AmericanAir'))\": False,\n",
       "  \"contains(('available', 'JetBlue'))\": False,\n",
       "  \"contains(('available', 'united'))\": False,\n",
       "  \"contains(('awesome', 'And'))\": False,\n",
       "  \"contains(('back', 'AmericanAir'))\": False,\n",
       "  \"contains(('back', 'SouthwestAir'))\": False,\n",
       "  \"contains(('back', 'united'))\": False,\n",
       "  \"contains(('best', 'part'))\": False,\n",
       "  \"contains(('boarding', 'flight'))\": False,\n",
       "  \"contains(('book', 'The'))\": False,\n",
       "  \"contains(('call', 'customer'))\": False,\n",
       "  \"contains(('captain', 'anything'))\": False,\n",
       "  \"contains(('change', 'flights'))\": False,\n",
       "  \"contains(('chocoLate', 'Flight'))\": False,\n",
       "  \"contains(('co', '9bzqZQx8DC'))\": False,\n",
       "  \"contains(('confirmation', 'code'))\": False,\n",
       "  \"contains(('could', 'fly'))\": False,\n",
       "  \"contains(('day', 'JetBlue'))\": False,\n",
       "  \"contains(('destination', 'dates'))\": False,\n",
       "  \"contains(('done', 'united'))\": False,\n",
       "  \"contains(('ever', 'SouthwestAir'))\": False,\n",
       "  \"contains(('every', 'day'))\": False,\n",
       "  \"contains(('exceptional', 'service'))\": False,\n",
       "  \"contains(('existing', 'reservation'))\": False,\n",
       "  \"contains(('finally', 'made'))\": False,\n",
       "  \"contains(('first', 'pair'))\": False,\n",
       "  \"contains(('flight', 'Award'))\": False,\n",
       "  \"contains(('flight', 'Not'))\": False,\n",
       "  \"contains(('flight', 'VirginAmerica'))\": False,\n",
       "  \"contains(('flight', 'jetblue'))\": False,\n",
       "  \"contains(('fly', 'Southwest'))\": False,\n",
       "  \"contains(('fly', 'southwest'))\": False,\n",
       "  \"contains(('flying', 'VirginAmerica'))\": False,\n",
       "  \"contains(('flying', 'united'))\": False,\n",
       "  \"contains(('flying', 'us'))\": False,\n",
       "  \"contains(('following', 'flight'))\": False,\n",
       "  \"contains(('free', 'trip'))\": False,\n",
       "  \"contains(('get', 'ahold'))\": False,\n",
       "  \"contains(('get', 'assistance'))\": False,\n",
       "  \"contains(('glad', 'u'))\": False,\n",
       "  \"contains(('good', 'Just'))\": False,\n",
       "  \"contains(('got', 'fixed'))\": False,\n",
       "  \"contains(('great', 'care'))\": False,\n",
       "  \"contains(('great', 'things'))\": False,\n",
       "  \"contains(('hear', 'back'))\": False,\n",
       "  \"contains(('help', 'changing'))\": False,\n",
       "  \"contains(('help', 'would'))\": False,\n",
       "  \"contains(('hey', 'guys'))\": False,\n",
       "  \"contains(('hey', 'southwest'))\": False,\n",
       "  \"contains(('home', 'tonight'))\": False,\n",
       "  \"contains(('hours', 'AmericanAir'))\": False,\n",
       "  \"contains(('huge', 'kudos'))\": False,\n",
       "  \"contains(('iad', 'Handed'))\": False,\n",
       "  \"contains(('imagine', 'dragons'))\": False,\n",
       "  \"contains(('info', 'JetBlue'))\": False,\n",
       "  \"contains(('job', 'celebrating'))\": False,\n",
       "  \"contains(('known', 'traveler'))\": False,\n",
       "  \"contains(('landing', 'ever'))\": False,\n",
       "  \"contains(('letting', 'us'))\": False,\n",
       "  \"contains(('lol', 'AmericanAir'))\": False,\n",
       "  \"contains(('lost', 'baggage'))\": False,\n",
       "  \"contains(('lost', 'luggage'))\": False,\n",
       "  \"contains(('love', 'JetBlue'))\": False,\n",
       "  \"contains(('loyal', 'customers'))\": False,\n",
       "  \"contains(('mom', 'left'))\": False,\n",
       "  \"contains(('murdering', 'ways'))\": False,\n",
       "  \"contains(('need', 'assistance'))\": False,\n",
       "  \"contains(('number', 'SouthwestAir'))\": False,\n",
       "  \"contains(('open', 'seats'))\": False,\n",
       "  \"contains(('phone', 'system'))\": False,\n",
       "  \"contains(('plane', 'JetBlue'))\": False,\n",
       "  \"contains(('play', 'ask'))\": False,\n",
       "  \"contains(('please', 'tell'))\": False,\n",
       "  \"contains(('pushed', 'back'))\": False,\n",
       "  \"contains(('return', 'trip'))\": False,\n",
       "  \"contains(('right', 'AmericanAir'))\": False,\n",
       "  \"contains(('right', 'SouthwestAir'))\": False,\n",
       "  \"contains(('rock', 'SouthwestAir'))\": False,\n",
       "  \"contains(('rock', 'united'))\": False,\n",
       "  \"contains(('saver', 'level'))\": False,\n",
       "  \"contains(('scavenger', 'hunt'))\": False,\n",
       "  \"contains(('second', 'time'))\": False,\n",
       "  \"contains(('service', 'VirginAmerica'))\": False,\n",
       "  \"contains(('service', 'amp'))\": False,\n",
       "  \"contains(('site', 'http'))\": False,\n",
       "  \"contains(('sjo', 'iad'))\": False,\n",
       "  \"contains(('smooth', 'flight'))\": False,\n",
       "  \"contains(('southwest', 'Can'))\": False,\n",
       "  \"contains(('still', 'love'))\": False,\n",
       "  \"contains(('still', 'nothing'))\": False,\n",
       "  \"contains(('thanks', 'VirginAmerica'))\": False,\n",
       "  \"contains(('thanks', 'We'))\": False,\n",
       "  \"contains(('think', 'http'))\": False,\n",
       "  \"contains(('time', 'http'))\": False,\n",
       "  \"contains(('told', 'us'))\": False,\n",
       "  \"contains(('tomorrow', 'Thanks'))\": False,\n",
       "  \"contains(('tonight', 'JetBlue'))\": False,\n",
       "  \"contains(('tonight', 'USAirways'))\": False,\n",
       "  \"contains(('took', 'great'))\": False,\n",
       "  \"contains(('traditions', 'alive'))\": False,\n",
       "  \"contains(('two', 'weeks'))\": False,\n",
       "  \"contains(('u', 'AmericanAir'))\": False,\n",
       "  \"contains(('united', 'Looking'))\": False,\n",
       "  \"contains(('united', 'USAirways'))\": False,\n",
       "  \"contains(('united', 'Your'))\": False,\n",
       "  \"contains(('united', 'also'))\": False,\n",
       "  \"contains(('united', 'give'))\": False,\n",
       "  \"contains(('united', 'good'))\": False,\n",
       "  \"contains(('united', 'huge'))\": False,\n",
       "  \"contains(('united', 'nope'))\": False,\n",
       "  \"contains(('united', 'staralliance'))\": False,\n",
       "  \"contains(('way', 'JetBlue'))\": False,\n",
       "  \"contains(('wings', 'Keeping'))\": False,\n",
       "  \"contains(('yr', 'old'))\": False,\n",
       "  \"contains(('1st', 'flight'))\": False,\n",
       "  \"contains(('2qJbCv5jzq', 'southwestairlines'))\": False,\n",
       "  \"contains(('About', 'Summer'))\": False,\n",
       "  \"contains(('Air', 'My'))\": False,\n",
       "  \"contains(('AirTahitiNui', 'amp'))\": False,\n",
       "  \"contains(('AirlineGeeks', 'avgeek'))\": False,\n",
       "  \"contains(('Alex', 'Espinosa'))\": False,\n",
       "  \"contains(('All', 'good'))\": False,\n",
       "  \"contains(('AmericanAir', 'AmericanAir'))\": False,\n",
       "  \"contains(('AmericanAir', 'How'))\": False,\n",
       "  \"contains(('AmericanAir', 'If'))\": False,\n",
       "  \"contains(('AmericanAir', 'keeping'))\": False,\n",
       "  \"contains(('AmericanAir', 'need'))\": False,\n",
       "  \"contains(('AmericanAir', 'oh'))\": False,\n",
       "  \"contains(('AmericanAir', 'thx'))\": False,\n",
       "  \"contains(('And', 'thank'))\": False,\n",
       "  \"contains(('Atlanta', 'Airport'))\": False,\n",
       "  \"contains(('Atlanta', 'My'))\": False,\n",
       "  \"contains(('Austin', 'Terry'))\": False,\n",
       "  \"contains(('Awesome', 'service'))\": False,\n",
       "  \"contains(('Aww', 'Thanks'))\": False,\n",
       "  \"contains(('Beach', 'Airport'))\": False,\n",
       "  \"contains(('Best', 'flight'))\": False,\n",
       "  \"contains(('Btw', 'excellent'))\": False,\n",
       "  \"contains(('Delta', 'SouthwestAir'))\": False,\n",
       "  \"contains(('Eliza', 'amp'))\": False,\n",
       "  \"contains(('Flight', 'JetBlue'))\": False,\n",
       "  \"contains(('Fort', 'Worth'))\": False,\n",
       "  \"contains(('FortuneMagazine', 'Love'))\": False,\n",
       "  \"contains(('Good', 'morning'))\": False,\n",
       "  \"contains(('Great', 'flight'))\": False,\n",
       "  \"contains(('Help', 'JetBlue'))\": False,\n",
       "  \"contains(('How', 'come'))\": False,\n",
       "  \"contains(('It', 'says'))\": False,\n",
       "  \"contains(('JetBlue', 'Can'))\": False,\n",
       "  \"contains(('JetBlue', 'Do'))\": False,\n",
       "  \"contains(('JetBlue', 'EllaHenderson'))\": False,\n",
       "  \"contains(('JetBlue', 'Has'))\": False,\n",
       "  \"contains(('JetBlue', 'Is'))\": False,\n",
       "  \"contains(('JetBlue', 'It'))\": False,\n",
       "  \"contains(('JetBlue', 'Wish'))\": False,\n",
       "  ...},\n",
       " 'negative')"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "#checking the length of the featureset\r\n",
    "#it should be the same than unigram, because we processed number of documents\r\n",
    "#this is for verification\r\n",
    "len(featuresets2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "#how we run the classifier to see if we get a good accuracy score\r\n",
    "#all this is just to find out which feature is more informative to predict sentiment\r\n",
    "#Naive Bayes classifier with 5-fold cross validation for training on sentiments using bigram features\r\n",
    "\r\n",
    "kf = KFold(n_splits = 10)\r\n",
    "sum = 0\r\n",
    "\r\n",
    "for train, test in kf.split(featuresets2):\r\n",
    "    train_data2 = np.array(featuresets2)[train]\r\n",
    "    test_data2 = np.array(featuresets2)[test]\r\n",
    "    classifier2 = nltk.NaiveBayesClassifier.train(train_data2)\r\n",
    "    sum += nltk.classify.accuracy(classifier2, test_data2)\r\n",
    "\r\n",
    "acc2 = sum/10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "#accuracy of classifier2, bigrams\r\n",
    "\r\n",
    "acc2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6092222222222221"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# determining the sentiments of the 1/4th of the comments using classifier1\r\n",
    "# for debugging purposes, we use only 1/100 of the corpus\r\n",
    "# but for the final experiment you should use at least 1/4 of it\r\n",
    "# NOTE: WITH 1/4 OF THE SAMPLE THIS CODE TAKES 2 HOURS TO RUN\r\n",
    "\r\n",
    "#creating emply lists to append the tweets to\r\n",
    "pos_sent = []\r\n",
    "neg_sent = []\r\n",
    "neu_sent = []\r\n",
    "\r\n",
    "#and total values of positive, negative or neutral tweets\r\n",
    "#all these lists will end up as columns in our csv file, created later on\r\n",
    "total_pos = []\r\n",
    "total_neg = []\r\n",
    "total_neu = []\r\n",
    "\r\n",
    "sentences = test_set\r\n",
    "    #opening the counter to add up positive, negative, or neutral according to predicted labels\r\n",
    "pos_count = 0\r\n",
    "neg_count = 0\r\n",
    "neu_count = 0\r\n",
    "#using our first classifier, the one trained with unigram features\r\n",
    "for sents in sentences:\r\n",
    "    senti = classifier.classify(document_features(sents, word_features))\r\n",
    "    #adding items to the counter as they are classified\r\n",
    "    if senti == 'positive':\r\n",
    "        pos_sent.append(sents)\r\n",
    "        pos_count += 1\r\n",
    "\r\n",
    "    elif senti == 'negative':\r\n",
    "        neg_sent.append(sents)\r\n",
    "        neg_count += 1\r\n",
    "    \r\n",
    "    else:\r\n",
    "        neu_sent.append(sents)\r\n",
    "        neu_count += 1\r\n",
    "\r\n",
    "    #appending the totals\r\n",
    "    total_pos.append(pos_count)\r\n",
    "    total_neg.append(neg_count)\r\n",
    "    total_neu.append(neu_count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "#how many positive sentiment sentences did we predict?\r\n",
    "len(pos_sent)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5635"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "#and how many negative sentiment ones?\r\n",
    "len(neg_sent)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6836"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "#what about neutral?\r\n",
    "len(neu_sent)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4433"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "#We tokenize and attach POS to each sentence in the Positive Sentences list\r\n",
    "tags_pos = [nltk.pos_tag(token) for token in pos_sent]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "#same for Negative Sentences\r\n",
    "\r\n",
    "tags_neg = [nltk.pos_tag(token) for token in neg_sent]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "tags_neu = [nltk.pos_tag(token) for token in neu_sent]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "#let's check that everything is okay. We print the first 3 Negative sentences\r\n",
    "print(tags_neg[:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[('The', 'DT'), ('wife', 'NN'), ('had', 'VBD'), ('discovered', 'VBN'), ('that', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('was', 'VBD'), ('carrying', 'VBG'), ('on', 'IN'), ('an', 'DT'), ('intrigue', 'NN'), ('with', 'IN'), ('a', 'DT'), ('French', 'JJ'), ('girl', 'NN'), (',', ','), ('who', 'WP'), ('had', 'VBD'), ('been', 'VBN'), ('a', 'DT'), ('governess', 'NN'), ('in', 'IN'), ('their', 'PRP$'), ('family', 'NN'), (',', ','), ('and', 'CC'), ('she', 'PRP'), ('had', 'VBD'), ('announced', 'VBN'), ('to', 'TO'), ('her', 'PRP$'), ('husband', 'NN'), ('that', 'IN'), ('she', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('go', 'VB'), ('on', 'IN'), ('living', 'NN'), ('in', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('house', 'NN'), ('with', 'IN'), ('him', 'PRP'), ('.', '.')], [('This', 'DT'), ('position', 'NN'), ('of', 'IN'), ('affairs', 'NNS'), ('had', 'VBD'), ('now', 'RB'), ('lasted', 'VBN'), ('three', 'CD'), ('days', 'NNS'), (',', ','), ('and', 'CC'), ('not', 'RB'), ('only', 'RB'), ('the', 'DT'), ('husband', 'NN'), ('and', 'CC'), ('wife', 'NN'), ('themselves', 'PRP'), (',', ','), ('but', 'CC'), ('all', 'PDT'), ('the', 'DT'), ('members', 'NNS'), ('of', 'IN'), ('their', 'PRP$'), ('family', 'NN'), ('and', 'CC'), ('household', 'NN'), (',', ','), ('were', 'VBD'), ('painfully', 'RB'), ('conscious', 'JJ'), ('of', 'IN'), ('it', 'PRP'), ('.', '.')], [('Every', 'DT'), ('person', 'NN'), ('in', 'IN'), ('the', 'DT'), ('house', 'NN'), ('felt', 'VBD'), ('that', 'IN'), ('there', 'EX'), ('was', 'VBD'), ('no', 'DT'), ('sense', 'NN'), ('in', 'IN'), ('their', 'PRP$'), ('living', 'NN'), ('together', 'RB'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('the', 'DT'), ('stray', 'JJ'), ('people', 'NNS'), ('brought', 'VBN'), ('together', 'RB'), ('by', 'IN'), ('chance', 'NN'), ('in', 'IN'), ('any', 'DT'), ('inn', 'NN'), ('had', 'VBD'), ('more', 'RBR'), ('in', 'IN'), ('common', 'JJ'), ('with', 'IN'), ('one', 'CD'), ('another', 'DT'), ('than', 'IN'), ('they', 'PRP'), (',', ','), ('the', 'DT'), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('family', 'NN'), ('and', 'CC'), ('household', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Oblonskys', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "#what about the first 3 positive ones...\r\n",
    "print(tags_pos[:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[('[', 'JJ'), ('Illustration', 'NNP'), (']', 'NNP'), ('ANNA', 'NNP'), ('KARENINA', 'NNP'), ('by', 'IN'), ('Leo', 'NNP'), ('Tolstoy', 'NNP'), ('Translated', 'VBN'), ('by', 'IN'), ('Constance', 'NNP'), ('Garnett', 'NNP'), ('Contents', 'NNP'), ('PART', 'NNP'), ('ONE', 'NNP'), ('PART', 'NNP'), ('TWO', 'NNP'), ('PART', 'NNP'), ('THREE', 'NNP'), ('PART', 'NNP'), ('FOUR', 'NNP'), ('PART', 'NNP'), ('FIVE', 'NNP'), ('PART', 'NNP'), ('SIX', 'NNP'), ('PART', 'NNP'), ('SEVEN', 'NNP'), ('PART', 'NNP'), ('EIGHT', 'NNP'), ('PART', 'NNP'), ('ONE', 'NNP'), ('Chapter', 'NN'), ('1', 'CD'), ('Happy', 'JJ'), ('families', 'NNS'), ('are', 'VBP'), ('all', 'DT'), ('alike', 'RB'), (';', ':'), ('every', 'DT'), ('unhappy', 'JJ'), ('family', 'NN'), ('is', 'VBZ'), ('unhappy', 'JJ'), ('in', 'IN'), ('its', 'PRP$'), ('own', 'JJ'), ('way', 'NN'), ('.', '.')], [('Everything', 'NN'), ('was', 'VBD'), ('in', 'IN'), ('confusion', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Oblonskys', 'NNP'), ('’', 'NNP'), ('house', 'NN'), ('.', '.')], [('“', 'RB'), ('Now', 'RB'), (',', ','), ('how', 'WRB'), ('was', 'VBD'), ('it', 'PRP'), ('?', '.')]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "# CREATING FUNCTIONS TO REUSE CODE FOR RETRIEVING DIFFERENTE TYPES OF PHRASES\r\n",
    "\r\n",
    "# We will apply this code to find ADJECTIVE PHRASES positive and negative sentences first\r\n",
    "# after that, you will need to change the grammar and subtree label in the function to extract:\r\n",
    "# ADVERB, NOUN, and VERB phrases, as stated in the Homework guidelines\r\n",
    "\r\n",
    "# EXTRACTING ACCORDING TO A GRAMMAR ADJECTIVE PHRASES\r\n",
    "# this is the code we reuse, passing the \"tags_pos\", \"tags_neg\" or \"tags_neu\" accordingly\r\n",
    "def grammar_phrases(tags_sent):\r\n",
    "    grammar_adjph = \"ADJPH: {<RB.?>+<JJ.?>}\" # REMEMBER TO EDI THIS GRAMMAR FOR ADVERBS & VERBS!\r\n",
    "    chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\r\n",
    "    adjph_tags = []\r\n",
    "    for sent in tags_sent:\r\n",
    "        if len(sent) > 0:\r\n",
    "            tree = chunk_parser_adj.parse(sent)\r\n",
    "            for subtree in tree.subtrees():\r\n",
    "                if subtree.label() == 'ADJPH': # THIS ALSO NEEDS EDITION FOR ADVERBS & VERBS\r\n",
    "                    adjph_tags.append(subtree)\r\n",
    "    return adjph_tags\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# EXTRACTING ADJECTIVE PHRASES without POS tags, just the phrase\r\n",
    "# we also reuse this but replacing the 'tagged_phrase' in each new case\r\n",
    "def word_phrase(tagged_phrase):\r\n",
    "    adjective_phrases = []\r\n",
    "    for sent in tagged_phrase:\r\n",
    "        temp = ''\r\n",
    "        for w, t in sent:\r\n",
    "            temp += w+ ' '    \r\n",
    "        adjective_phrases.append(temp)\r\n",
    "    return adjective_phrases\r\n",
    "\r\n",
    "\r\n",
    "# RANKING BY FREQUENCY\r\n",
    "# this is also a function to reuse\r\n",
    "def get_frequency(phrases):\r\n",
    "    phrases_frequency = nltk.FreqDist(phrases)\r\n",
    "    return phrases_frequency.most_common(50)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "#how many positive sentiment sentences did we predict?\r\n",
    "len(pos_sent)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5635"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# EXTRACTING POSITIVE PHRASES AND THEIR POS\r\n",
    "adjph_pos = grammar_phrases(tags_pos)\r\n",
    "print('Adjective phrases in positive sentences, with POS: ', adjph_pos[:3])\r\n",
    "\r\n",
    "# EXTRACTING POSITIVE ADJECTIVE PHRASES (WORDS ONLY)\r\n",
    "word_adjph_pos = word_phrase(adjph_pos)\r\n",
    "print('First 10 adjective phrases in positive sentences: ', word_adjph_pos[:10])\r\n",
    "\r\n",
    "# RANKING POSITIVE PHRASES BY FREQUENCY\r\n",
    "most_common_adjph_pos = get_frequency(word_adjph_pos)\r\n",
    "print(\"Top 50 adjective phrases in positive sentences: \", most_common_adjph_pos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adjective phrases in positive sentences, with POS:  [Tree('ADJPH', [('very', 'RB'), ('nice', 'JJ')]), Tree('ADJPH', [('sly', 'RB'), ('smile', 'JJ')]), Tree('ADJPH', [('rather', 'RB'), ('pitiful', 'JJ')])]\n",
      "First 10 adjective phrases in positive sentences:  ['very nice ', 'sly smile ', 'rather pitiful ', 'very unpleasant ', 'still damp ', '“ Good ', 'once luxuriant ', 'unutterably sorry ', 'more heated ', 'very likely ']\n",
      "Top 50 adjective phrases in positive sentences:  [('very glad ', 30), ('so much ', 20), ('too much ', 13), ('very nice ', 9), ('very good ', 8), ('so glad ', 8), ('very much ', 7), ('very interesting ', 5), ('“ Good ', 4), ('so happy ', 4), ('very bad ', 4), ('Very good ', 4), ('s jealous ', 4), ('Very glad ', 4), ('very grateful ', 4), ('very instant ', 4), ('even more ', 4), ('so little ', 4), ('s good ', 3), ('very intelligent ', 3), ('so good ', 3), ('once more ', 3), ('very young ', 3), ('very sweet ', 3), ('not true ', 3), ('not good ', 3), ('most various ', 3), ('ever so much ', 3), ('more excited ', 3), ('very best ', 3), ('too gloomy ', 3), ('very attractive ', 3), ('so strange ', 3), ('sly smile ', 2), ('scarcely perceptible ', 2), ('hardly perceptible ', 2), ('too great ', 2), ('Very likely ', 2), ('very simple ', 2), ('so distressed ', 2), ('however much ', 2), ('m glad ', 2), ('so stupid ', 2), ('most important ', 2), ('so nice ', 2), ('very different ', 2), ('very sorry ', 2), ('too late ', 2), ('not angry ', 2), ('most favorable ', 2)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "# NOW, YOUR TURN!!\r\n",
    "# USE THESE FUNCTIONS TO EXTRACT NEGATIVE PHRASES, CLEAN NEGATIVE ADJECTIVE PHRASES FROM POS,\r\n",
    "# AND RANK NEGATIVE PHRASES BY FREQUENCY\r\n",
    "\r\n",
    "# YOU ALSO NEED TO MODIFY THE GRAMMAR TO LOOK FOR:\r\n",
    "# NOUNS\r\n",
    "# VERBS\r\n",
    "## IN BOTH CASES, DON'T FORGET THAT THERE ARE MANY TAGS IN EACH GROUP!!\r\n",
    "##  CHECK THE PENN POS TAGS LIST: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "adjph_neg = grammar_phrases(tags_neg)\r\n",
    "print('Adjective pharases in negative sentences, with POS: ', adjph_neg[:3])\r\n",
    "\r\n",
    "word_adjph_neg = word_phrase(adjph_neg)\r\n",
    "print('First 10 adjective phrases in negative sentences: ', word_adjph_neg[:10])\r\n",
    "\r\n",
    "most_common_adjph_neg = get_frequency(word_adjph_neg)\r\n",
    "print('Top 50 adjective phrarses in negative sentences: ', most_common_adjph_neg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adjective pharases in negative sentences, with POS:  [Tree('ADJPH', [('painfully', 'RB'), ('conscious', 'JJ')]), Tree('ADJPH', [('most', 'RBS'), ('awful', 'JJ')]), Tree('ADJPH', [('not', 'RB'), ('so', 'RB'), ('much', 'JJ')])]\n",
      "First 10 adjective phrases in negative sentences:  ['painfully conscious ', 'most awful ', 'not so much ', 'very disgraceful ', 'therefore idiotic ', 'no longer young ', 'most complex ', 'so fond ', 'absolutely essential ', 'most unpleasant ']\n",
      "Top 50 adjective phrarses in negative sentences:  [('so much ', 41), ('so many ', 22), ('very glad ', 14), ('once more ', 12), ('most important ', 11), ('not so much ', 10), ('so awful ', 10), ('very nice ', 9), ('very good ', 9), ('too much ', 9), ('very sorry ', 8), ('as much ', 8), ('so little ', 7), ('utterly unable ', 7), ('too late ', 6), ('so good ', 6), ('so happy ', 6), ('very interesting ', 6), ('very busy ', 5), ('very much ', 5), ('quite different ', 5), ('very instant ', 5), ('not afraid ', 5), ('so glad ', 5), ('still more ', 5), ('more important ', 5), ('so terrible ', 4), ('s impossible ', 4), ('so difficult ', 4), ('so clear ', 4), ('utterly impossible ', 4), ('very well aware ', 4), ('not nice ', 4), ('very same ', 4), ('not angry ', 4), ('not last ', 4), ('most difficult ', 4), ('most intimate ', 4), ('not wrong ', 4), ('very true ', 4), ('well aware ', 4), ('very grateful ', 4), ('so simple ', 4), ('painfully conscious ', 3), ('most awful ', 3), ('so fond ', 3), ('most unpleasant ', 3), ('still worse ', 3), ('just as much ', 3), ('much interested ', 3)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nouns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "def grammar_phrases(tags_sent):\r\n",
    "    grammar_adjph = \"NNPH: {<JJ.?>+<NN.?>}\" # REMEMBER TO EDI THIS GRAMMAR FOR ADVERBS & VERBS!\r\n",
    "    chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\r\n",
    "    adjph_tags = []\r\n",
    "    for sent in tags_sent:\r\n",
    "        if len(sent) > 0:\r\n",
    "            tree = chunk_parser_adj.parse(sent)\r\n",
    "            for subtree in tree.subtrees():\r\n",
    "                if subtree.label() == 'NNPH': # THIS ALSO NEEDS EDITION FOR ADVERBS & VERBS\r\n",
    "                    adjph_tags.append(subtree)\r\n",
    "    return adjph_tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# EXTRACTING POSITIVE PHRASES AND THEIR POS\r\n",
    "adjph_pos = grammar_phrases(tags_pos)\r\n",
    "print('noun phrases in positive sentences, with POS: ', adjph_pos[:3])\r\n",
    "\r\n",
    "# EXTRACTING POSITIVE ADJECTIVE PHRASES (WORDS ONLY)\r\n",
    "word_adjph_pos = word_phrase(adjph_pos)\r\n",
    "print('First 10 noun phrases in positive sentences: ', word_adjph_pos[:10])\r\n",
    "\r\n",
    "# RANKING POSITIVE PHRASES BY FREQUENCY\r\n",
    "most_common_adjph_pos = get_frequency(word_adjph_pos)\r\n",
    "print(\"Top 50 Noun phrases in positive sentences: \", most_common_adjph_pos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "noun phrases in positive sentences, with POS:  [Tree('NNPH', [('[', 'JJ'), ('Illustration', 'NNP')]), Tree('NNPH', [('Happy', 'JJ'), ('families', 'NNS')]), Tree('NNPH', [('unhappy', 'JJ'), ('family', 'NN')])]\n",
      "First 10 noun phrases in positive sentences:  ['[ Illustration ', 'Happy families ', 'unhappy family ', 'own way ', 'whole situation ', 'idiotic smile ', 'other way ', 'oh dear ', 'oh dear ', 's bad _her_ ']\n",
      "Top 50 Noun phrases in positive sentences:  [('“ Yes ', 58), ('’ t ', 57), ('” “ ', 40), ('“ Ah ', 24), ('’ s ', 20), ('great deal ', 20), ('.... ” ', 14), ('same time ', 14), ('“ Come ', 13), ('old man ', 13), ('“ Thank ', 12), ('“ Very ', 12), ('” “ Yes ', 9), ('“ Don ', 8), ('young man ', 8), ('first time ', 8), ('next day ', 8), ('other people ', 7), ('good spirits ', 7), ('_ ” ', 7), ('little girl ', 6), ('s voice ', 6), ('other side ', 6), ('long while ', 6), ('m glad ', 6), ('same thing ', 6), ('s eyes ', 6), ('” Kitty ', 6), ('sick man ', 6), ('handsome face ', 5), ('young men ', 5), ('“ Let ', 5), ('s face ', 5), ('strange feeling ', 5), ('s wife ', 5), ('“ No ', 5), ('“ Alexey ', 5), ('’ clock ', 5), ('good humor ', 4), ('perceptible smile ', 4), ('ve come ', 4), ('now. ” ', 4), ('little hand ', 4), ('it. ” ', 4), ('new life ', 4), ('last year ', 4), ('s hand ', 4), ('“ Look ', 4), ('nice person ', 4), ('first moment ', 4)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "adjph_neg = grammar_phrases(tags_neg)\r\n",
    "print('Noun pharases in negative sentences, with POS: ', adjph_neg[:3])\r\n",
    "\r\n",
    "word_adjph_neg = word_phrase(adjph_neg)\r\n",
    "print('First 10 noun phrases in negative sentences: ', word_adjph_neg[:10])\r\n",
    "\r\n",
    "most_common_adjph_neg = get_frequency(word_adjph_neg)\r\n",
    "print('Top 50 noun phrarses in negative sentences: ', most_common_adjph_neg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Noun pharases in negative sentences, with POS:  [Tree('NNPH', [('French', 'JJ'), ('girl', 'NN')]), Tree('NNPH', [('same', 'JJ'), ('house', 'NN')]), Tree('NNPH', [('stray', 'JJ'), ('people', 'NNS')])]\n",
      "First 10 noun phrases in negative sentences:  ['French girl ', 'same house ', 'stray people ', 'own room ', 'new situation ', 'dinner time ', 'usual hour ', '’ clock ', 'leather-covered sofa ', 'well-cared-for person ']\n",
      "Top 50 noun phrarses in negative sentences:  [('’ t ', 402), ('” “ ', 223), ('“ Yes ', 49), ('same time ', 46), ('.... ” ', 43), ('old man ', 40), ('’ s ', 40), ('long while ', 38), ('first time ', 32), ('it. ” ', 31), ('great deal ', 29), ('sick man ', 28), ('” Levin ', 27), ('other side ', 25), ('other people ', 25), ('old prince ', 23), ('several times ', 22), ('young man ', 21), ('next day ', 19), ('same thing ', 19), ('“ Come ', 19), ('few words ', 15), ('only thing ', 13), ('me. ” ', 12), ('little girl ', 12), ('same way ', 12), ('’ clock ', 11), ('him. ” ', 11), ('young men ', 11), ('s voice ', 11), ('o ’ ', 10), ('whole life ', 10), ('m afraid ', 10), ('” “ Yes ', 10), ('“ Don ', 10), ('previous day ', 10), ('same position ', 10), ('you. ” ', 9), ('” Alexey ', 9), ('own room ', 8), ('next room ', 8), ('young people ', 8), ('peasant women ', 8), ('s face ', 8), ('now. ” ', 8), ('first minute ', 7), ('many times ', 7), ('s eyes ', 7), ('” Kitty ', 7), ('s hand ', 7)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verbs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "def grammar_phrases(tags_sent):\r\n",
    "    grammar_adjph = \"VBPH: {<RB.?>+<VB.?>}\" # REMEMBER TO EDI THIS GRAMMAR FOR ADVERBS & VERBS!\r\n",
    "    chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\r\n",
    "    adjph_tags = []\r\n",
    "    for sent in tags_sent:\r\n",
    "        if len(sent) > 0:\r\n",
    "            tree = chunk_parser_adj.parse(sent)\r\n",
    "            for subtree in tree.subtrees():\r\n",
    "                if subtree.label() == 'VBPH': # THIS ALSO NEEDS EDITION FOR ADVERBS & VERBS\r\n",
    "                    adjph_tags.append(subtree)\r\n",
    "    return adjph_tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# EXTRACTING POSITIVE PHRASES AND THEIR POS\r\n",
    "adjph_pos = grammar_phrases(tags_pos)\r\n",
    "print('Verb phrases in positive sentences, with POS: ', adjph_pos[:3])\r\n",
    "\r\n",
    "# EXTRACTING POSITIVE ADJECTIVE PHRASES (WORDS ONLY)\r\n",
    "word_adjph_pos = word_phrase(adjph_pos)\r\n",
    "print('First 10 verb phrases in positive sentences: ', word_adjph_pos[:10])\r\n",
    "\r\n",
    "# RANKING POSITIVE PHRASES BY FREQUENCY\r\n",
    "most_common_adjph_pos = get_frequency(word_adjph_pos)\r\n",
    "print(\"Top 50 verb phrases in positive sentences: \", most_common_adjph_pos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Verb phrases in positive sentences, with POS:  [Tree('VBPH', [('not', 'RB'), ('forgive', 'VB')]), Tree('VBPH', [('vividly', 'RB'), ('recalled', 'VBD')]), Tree('VBPH', [('merely', 'RB'), ('glanced', 'VBD')])]\n",
      "First 10 verb phrases in positive sentences:  ['not forgive ', 'vividly recalled ', 'merely glanced ', 'obviously prepared ', 'always are ', 'dress me. ', 'once recalled ', '” answered ', 'once perceived ', 'still stroking ']\n",
      "Top 50 verb phrases in positive sentences:  [('” answered ', 19), ('ve been ', 16), ('not help ', 9), ('not be ', 9), ('not hear ', 6), ('not know ', 6), ('not go ', 6), ('too was ', 6), ('not understand ', 5), ('always been ', 5), ('always did ', 5), ('ll be ', 5), ('never seen ', 5), ('not believe ', 4), ('“ Delighted ', 4), ('there was ', 4), ('not speak ', 4), ('never taking ', 4), ('really was ', 4), ('not have ', 4), ('just been ', 4), ('vividly recalled ', 3), ('always excited ', 3), ('not resist ', 3), ('ve found ', 3), ('not come ', 3), ('not sleep ', 3), ('not been ', 3), ('not say ', 3), ('ll tell ', 3), ('not see ', 3), ('not take ', 3), ('not care ', 3), ('always felt ', 3), ('ever seen ', 3), ('ll go ', 3), ('not exactly ashamed ', 3), ('not love ', 2), ('completely recovered ', 2), ('often did ', 2), ('not making ', 2), ('well known ', 2), ('not knowing ', 2), ('not taking ', 2), ('not bear ', 2), ('scarcely remembered ', 2), ('once got ', 2), ('involuntarily listening ', 2), ('never be ', 2), ('perfectly composed ', 2)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "adjph_neg = grammar_phrases(tags_neg)\r\n",
    "print('Verb pharases in negative sentences, with POS: ', adjph_neg[:3])\r\n",
    "\r\n",
    "word_adjph_neg = word_phrase(adjph_neg)\r\n",
    "print('First 10 verb phrases in negative sentences: ', word_adjph_neg[:10])\r\n",
    "\r\n",
    "most_common_adjph_neg = get_frequency(word_adjph_neg)\r\n",
    "print('Top 50 verb phrarses in negative sentences: ', most_common_adjph_neg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Verb pharases in negative sentences, with POS:  [Tree('VBPH', [('not', 'RB'), ('go', 'VB')]), Tree('VBPH', [('now', 'RB'), ('lasted', 'VBN')]), Tree('VBPH', [('not', 'RB'), ('leave', 'VB')])]\n",
      "First 10 verb phrases in negative sentences:  ['not go ', 'now lasted ', 'not leave ', 'not been ', 'vigorously embraced ', 'even expressing ', 'cheerfully dropped ', 'suddenly remembered ', 'not sleeping ', 'not found ']\n",
      "Top 50 verb phrarses in negative sentences:  [('not be ', 96), ('not know ', 61), ('not been ', 48), ('not help ', 44), ('not have ', 40), ('ve been ', 37), ('not go ', 35), ('not understand ', 28), ('not want ', 25), ('not come ', 22), ('there was ', 20), ('not see ', 20), ('not believe ', 19), ('always did ', 18), ('not knowing ', 18), ('not going ', 18), ('not take ', 17), ('not speak ', 16), ('not think ', 16), ('not looking ', 15), ('” answered ', 15), ('not get ', 14), ('not say ', 14), ('not care ', 14), ('long been ', 13), ('never seen ', 13), ('always been ', 13), ('not seen ', 13), ('not answer ', 12), ('not give ', 12), ('not make ', 12), ('never have ', 12), ('not tell ', 12), ('not hear ', 12), ('not having ', 12), ('not let ', 11), ('never be ', 11), ('only be ', 11), ('m going ', 11), ('never had ', 10), ('not find ', 10), ('not love ', 9), ('not live ', 9), ('not look ', 9), ('never been ', 9), ('not agree ', 9), ('ll be ', 8), ('always had ', 8), ('t be ', 8), ('soon be ', 8)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "tagged_data=[]\r\n",
    "\r\n",
    "for i in range(0, len(pos_sent)):\r\n",
    "    group = {'Text':pos_sent[i], \"Sentiment\":\"Positive\", \"Book\":\"Anna Karenina\", \"Author\":\"Leo Tolstoy\", \"Total Postive Sentences\": pos_count, 'Total Negative Sentences':neg_count, 'Total Neutral Sentences':neu_count}\r\n",
    "    tagged_data.append(group)\r\n",
    "for i in range(0, len(neg_sent)):\r\n",
    "    group = {'Text':neg_sent[i], 'Sentiment':'Negative', \"Book\":\"Anna Karenina\", \"Author\":\"Leo Tolstoy\", \"Total Postive Sentences\": pos_count, 'Total Negative Sentences':neg_count, 'Total Neutral Sentences':neu_count}\r\n",
    "    tagged_data.append(group)\r\n",
    "for i in range(0, len(neu_sent)):\r\n",
    "    group = {'Text':neu_sent[i], 'Sentiment':'Neutral', \"Book\":\"Anna Karenina\",\"Author\":\"Leo Tolstoy\", \"Total Postive Sentences\": pos_count, 'Total Negative Sentences':neg_count, 'Total Neutral Sentences':neu_count}\r\n",
    "    tagged_data.append(group)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Great Gatsby"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "#call The Great Gatsby from Gutenberg Library\r\n",
    "test_set = strip_headers(load_etext(64317)).strip()\r\n",
    "\r\n",
    "#Split the Sentences into Tokens\r\n",
    "TextToken = nltk.sent_tokenize(test_set)\r\n",
    "TextToken = TextToken[2:] # Removes unrelated text at the front about the Gutenberg Project\r\n",
    "print(TextToken[:5])\r\n",
    "SentTokens = [nltk.word_tokenize(sent) for sent in TextToken] #Tokenize each word in the sentence\r\n",
    "print(SentTokens[:5])\r\n",
    "test_set = SentTokens\r\n",
    "\r\n",
    "# To Save Memory, removing the additional data sets\r\n",
    "del TextToken\r\n",
    "del SentTokens"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['In consequence, I’m inclined to reserve all judgements, a\\r\\nhabit that has opened up many curious natures to me and also made me\\r\\nthe victim of not a few veteran bores.', 'The abnormal mind is quick to\\r\\ndetect and attach itself to this quality when it appears in a normal\\r\\nperson, and so it came about that in college I was unjustly accused of\\r\\nbeing a politician, because I was privy to the secret griefs of wild,\\r\\nunknown men.', 'Most of the confidences were unsought—frequently I have\\r\\nfeigned sleep, preoccupation, or a hostile levity when I realized by\\r\\nsome unmistakable sign that an intimate revelation was quivering on\\r\\nthe horizon; for the intimate revelations of young men, or at least\\r\\nthe terms in which they express them, are usually plagiaristic and\\r\\nmarred by obvious suppressions.', 'Reserving judgements is a matter of\\r\\ninfinite hope.', 'I am still a little afraid of missing something if I\\r\\nforget that, as my father snobbishly suggested, and I snobbishly\\r\\nrepeat, a sense of the fundamental decencies is parcelled out\\r\\nunequally at birth.']\n",
      "[['In', 'consequence', ',', 'I', '’', 'm', 'inclined', 'to', 'reserve', 'all', 'judgements', ',', 'a', 'habit', 'that', 'has', 'opened', 'up', 'many', 'curious', 'natures', 'to', 'me', 'and', 'also', 'made', 'me', 'the', 'victim', 'of', 'not', 'a', 'few', 'veteran', 'bores', '.'], ['The', 'abnormal', 'mind', 'is', 'quick', 'to', 'detect', 'and', 'attach', 'itself', 'to', 'this', 'quality', 'when', 'it', 'appears', 'in', 'a', 'normal', 'person', ',', 'and', 'so', 'it', 'came', 'about', 'that', 'in', 'college', 'I', 'was', 'unjustly', 'accused', 'of', 'being', 'a', 'politician', ',', 'because', 'I', 'was', 'privy', 'to', 'the', 'secret', 'griefs', 'of', 'wild', ',', 'unknown', 'men', '.'], ['Most', 'of', 'the', 'confidences', 'were', 'unsought—frequently', 'I', 'have', 'feigned', 'sleep', ',', 'preoccupation', ',', 'or', 'a', 'hostile', 'levity', 'when', 'I', 'realized', 'by', 'some', 'unmistakable', 'sign', 'that', 'an', 'intimate', 'revelation', 'was', 'quivering', 'on', 'the', 'horizon', ';', 'for', 'the', 'intimate', 'revelations', 'of', 'young', 'men', ',', 'or', 'at', 'least', 'the', 'terms', 'in', 'which', 'they', 'express', 'them', ',', 'are', 'usually', 'plagiaristic', 'and', 'marred', 'by', 'obvious', 'suppressions', '.'], ['Reserving', 'judgements', 'is', 'a', 'matter', 'of', 'infinite', 'hope', '.'], ['I', 'am', 'still', 'a', 'little', 'afraid', 'of', 'missing', 'something', 'if', 'I', 'forget', 'that', ',', 'as', 'my', 'father', 'snobbishly', 'suggested', ',', 'and', 'I', 'snobbishly', 'repeat', ',', 'a', 'sense', 'of', 'the', 'fundamental', 'decencies', 'is', 'parcelled', 'out', 'unequally', 'at', 'birth', '.']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "# determining the sentiments of the 1/4th of the comments using classifier1\r\n",
    "# for debugging purposes, we use only 1/100 of the corpus\r\n",
    "# but for the final experiment you should use at least 1/4 of it\r\n",
    "# NOTE: WITH 1/4 OF THE SAMPLE THIS CODE TAKES 2 HOURS TO RUN\r\n",
    "\r\n",
    "#creating emply lists to append the tweets to\r\n",
    "pos_sent = []\r\n",
    "neg_sent = []\r\n",
    "neu_sent = []\r\n",
    "\r\n",
    "#and total values of positive, negative or neutral tweets\r\n",
    "#all these lists will end up as columns in our csv file, created later on\r\n",
    "total_pos = []\r\n",
    "total_neg = []\r\n",
    "total_neu = []\r\n",
    "\r\n",
    "sentences = test_set\r\n",
    "    #opening the counter to add up positive, negative, or neutral according to predicted labels\r\n",
    "pos_count = 0\r\n",
    "neg_count = 0\r\n",
    "neu_count = 0\r\n",
    "#using our first classifier, the one trained with unigram features\r\n",
    "for sents in sentences:\r\n",
    "    senti = classifier.classify(document_features(sents, word_features))\r\n",
    "    #adding items to the counter as they are classified\r\n",
    "    if senti == 'positive':\r\n",
    "        pos_sent.append(sents)\r\n",
    "        pos_count += 1\r\n",
    "\r\n",
    "    elif senti == 'negative':\r\n",
    "        neg_sent.append(sents)\r\n",
    "        neg_count += 1\r\n",
    "    \r\n",
    "    else:\r\n",
    "        neu_sent.append(sents)\r\n",
    "        neu_count += 1\r\n",
    "\r\n",
    "    #appending the totals\r\n",
    "    total_pos.append(pos_count)\r\n",
    "    total_neg.append(neg_count)\r\n",
    "    total_neu.append(neu_count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "#how many positive sentiment sentences did we predict?\r\n",
    "print(\"Number of Positive Sentences: \",len(pos_sent))\r\n",
    "print(\"Number of Negative Sentences: \",len(neg_sent))\r\n",
    "print(\"Number of Neutral Sentences: \",len(neu_sent))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Positive Sentences:  817\n",
      "Number of Negative Sentences:  948\n",
      "Number of Neutral Sentences:  672\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "tags_pos = [nltk.pos_tag(token) for token in pos_sent]\r\n",
    "tags_neg = [nltk.pos_tag(token) for token in neg_sent]\r\n",
    "tags_neu = [nltk.pos_tag(token) for token in neu_sent]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "#Negative Tags\r\n",
    "print(tags_neg[:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[('In', 'IN'), ('consequence', 'NN'), (',', ','), ('I', 'PRP'), ('’', 'VBP'), ('m', 'RB'), ('inclined', 'VBN'), ('to', 'TO'), ('reserve', 'VB'), ('all', 'DT'), ('judgements', 'NNS'), (',', ','), ('a', 'DT'), ('habit', 'NN'), ('that', 'WDT'), ('has', 'VBZ'), ('opened', 'VBN'), ('up', 'RP'), ('many', 'JJ'), ('curious', 'JJ'), ('natures', 'NNS'), ('to', 'TO'), ('me', 'PRP'), ('and', 'CC'), ('also', 'RB'), ('made', 'VBD'), ('me', 'PRP'), ('the', 'DT'), ('victim', 'NN'), ('of', 'IN'), ('not', 'RB'), ('a', 'DT'), ('few', 'JJ'), ('veteran', 'NN'), ('bores', 'NNS'), ('.', '.')], [('The', 'DT'), ('abnormal', 'JJ'), ('mind', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('to', 'TO'), ('detect', 'VB'), ('and', 'CC'), ('attach', 'VB'), ('itself', 'PRP'), ('to', 'TO'), ('this', 'DT'), ('quality', 'NN'), ('when', 'WRB'), ('it', 'PRP'), ('appears', 'VBZ'), ('in', 'IN'), ('a', 'DT'), ('normal', 'JJ'), ('person', 'NN'), (',', ','), ('and', 'CC'), ('so', 'RB'), ('it', 'PRP'), ('came', 'VBD'), ('about', 'IN'), ('that', 'DT'), ('in', 'IN'), ('college', 'NN'), ('I', 'PRP'), ('was', 'VBD'), ('unjustly', 'RB'), ('accused', 'VBN'), ('of', 'IN'), ('being', 'VBG'), ('a', 'DT'), ('politician', 'NN'), (',', ','), ('because', 'IN'), ('I', 'PRP'), ('was', 'VBD'), ('privy', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('secret', 'JJ'), ('griefs', 'NN'), ('of', 'IN'), ('wild', 'JJ'), (',', ','), ('unknown', 'JJ'), ('men', 'NNS'), ('.', '.')], [('Most', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('confidences', 'NNS'), ('were', 'VBD'), ('unsought—frequently', 'RB'), ('I', 'PRP'), ('have', 'VBP'), ('feigned', 'VBN'), ('sleep', 'NN'), (',', ','), ('preoccupation', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('hostile', 'JJ'), ('levity', 'NN'), ('when', 'WRB'), ('I', 'PRP'), ('realized', 'VBN'), ('by', 'IN'), ('some', 'DT'), ('unmistakable', 'JJ'), ('sign', 'NN'), ('that', 'IN'), ('an', 'DT'), ('intimate', 'JJ'), ('revelation', 'NN'), ('was', 'VBD'), ('quivering', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('horizon', 'NN'), (';', ':'), ('for', 'IN'), ('the', 'DT'), ('intimate', 'JJ'), ('revelations', 'NNS'), ('of', 'IN'), ('young', 'JJ'), ('men', 'NNS'), (',', ','), ('or', 'CC'), ('at', 'IN'), ('least', 'JJS'), ('the', 'DT'), ('terms', 'NNS'), ('in', 'IN'), ('which', 'WDT'), ('they', 'PRP'), ('express', 'VBP'), ('them', 'PRP'), (',', ','), ('are', 'VBP'), ('usually', 'RB'), ('plagiaristic', 'JJ'), ('and', 'CC'), ('marred', 'VBN'), ('by', 'IN'), ('obvious', 'JJ'), ('suppressions', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "#Postive Tags\r\n",
    "print(tags_pos[:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[('Reserving', 'VBG'), ('judgements', 'NNS'), ('is', 'VBZ'), ('a', 'DT'), ('matter', 'NN'), ('of', 'IN'), ('infinite', 'JJ'), ('hope', 'NN'), ('.', '.')], [('I', 'PRP'), ('graduated', 'VBD'), ('from', 'IN'), ('New', 'NNP'), ('Haven', 'NNP'), ('in', 'IN'), ('1915', 'CD'), (',', ','), ('just', 'RB'), ('a', 'DT'), ('quarter', 'NN'), ('of', 'IN'), ('a', 'DT'), ('century', 'NN'), ('after', 'IN'), ('my', 'PRP$'), ('father', 'NN'), (',', ','), ('and', 'CC'), ('a', 'DT'), ('little', 'JJ'), ('later', 'JJ'), ('I', 'PRP'), ('participated', 'VBD'), ('in', 'IN'), ('that', 'DT'), ('delayed', 'VBD'), ('Teutonic', 'NNP'), ('migration', 'NN'), ('known', 'VBN'), ('as', 'IN'), ('the', 'DT'), ('Great', 'NNP'), ('War', 'NNP'), ('.', '.')], [('I', 'PRP'), ('enjoyed', 'VBD'), ('the', 'DT'), ('counter-raid', 'NN'), ('so', 'RB'), ('thoroughly', 'RB'), ('that', 'IN'), ('I', 'PRP'), ('came', 'VBD'), ('back', 'RB'), ('restless', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gatsby Adjective Tags"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "def grammar_phrases(tags_sent):\r\n",
    "    grammar_adjph = \"ADJPH: {<RB.?>+<JJ.?>}\" # REMEMBER TO EDI THIS GRAMMAR FOR ADVERBS & VERBS!\r\n",
    "    chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\r\n",
    "    adjph_tags = []\r\n",
    "    for sent in tags_sent:\r\n",
    "        if len(sent) > 0:\r\n",
    "            tree = chunk_parser_adj.parse(sent)\r\n",
    "            for subtree in tree.subtrees():\r\n",
    "                if subtree.label() == 'ADJPH': # THIS ALSO NEEDS EDITION FOR ADVERBS & VERBS\r\n",
    "                    adjph_tags.append(subtree)\r\n",
    "    return adjph_tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "# EXTRACTING POSITIVE PHRASES AND THEIR POS\r\n",
    "adjph_pos = grammar_phrases(tags_pos)\r\n",
    "print('Adjective phrases in positive sentences, with POS: ', adjph_pos[:3])\r\n",
    "\r\n",
    "# EXTRACTING POSITIVE ADJECTIVE PHRASES (WORDS ONLY)\r\n",
    "word_adjph_pos = word_phrase(adjph_pos)\r\n",
    "print('First 10 adjective phrases in positive sentences: ', word_adjph_pos[:10])\r\n",
    "\r\n",
    "# RANKING POSITIVE PHRASES BY FREQUENCY\r\n",
    "most_common_adjph_pos = get_frequency(word_adjph_pos)\r\n",
    "print(\"Top 50 adjective phrases in positive sentences: \", most_common_adjph_pos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adjective phrases in positive sentences, with POS:  [Tree('ADJPH', [('most', 'RBS'), ('domesticated', 'JJ')]), Tree('ADJPH', [('rather', 'RB'), ('hard', 'JJ')]), Tree('ADJPH', [('as', 'RB'), ('much', 'JJ')])]\n",
      "First 10 adjective phrases in positive sentences:  ['most domesticated ', 'rather hard ', 'as much ', 'as cool ', 'rather impressive ', '” insisted ', 'forward unashamed ', 'very bad ', 'pretty cynical ', 'most advanced ']\n",
      "Top 50 adjective phrases in positive sentences:  [('very good ', 3), ('“ Good ', 2), ('too much ', 2), ('so hot ', 2), ('most domesticated ', 1), ('rather hard ', 1), ('as much ', 1), ('as cool ', 1), ('rather impressive ', 1), ('” insisted ', 1), ('forward unashamed ', 1), ('very bad ', 1), ('pretty cynical ', 1), ('most advanced ', 1), ('yard high ', 1), ('faintly handsome ', 1), ('rather wide ', 1), ('very beautiful ', 1), ('so remarkable ', 1), ('surprisingly formal ', 1), ('too big ', 1), ('absolutely real ', 1), ('somewhere last ', 1), ('“ Much ', 1), ('understandingly—much more ', 1), ('so intimate ', 1), ('so much ', 1), ('highly indignant ', 1), ('most amazing ', 1), ('right there … Good ', 1), ('very little ', 1), ('incurably dishonest ', 1), ('just as careless ', 1), ('very hard ', 1), ('not much ', 1), ('oddly familiar ', 1), ('very interesting ', 1), ('very careful ', 1), ('very polite ', 1), ('very sentimental ', 1), ('slightly older ', 1), ('absolutely perfect ', 1), ('too late. ', 1), ('Once more ', 1), ('up … ', 1), ('once more ', 1), ('most grotesque ', 1), ('too hospitable ', 1), ('indirectly due ', 1), ('so little ', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "# EXTRACTING POSITIVE PHRASES AND THEIR POS\r\n",
    "adjph_pos = grammar_phrases(tags_neg)\r\n",
    "print('Adjective phrases in positive sentences, with POS: ', adjph_pos[:3])\r\n",
    "\r\n",
    "# EXTRACTING POSITIVE ADJECTIVE PHRASES (WORDS ONLY)\r\n",
    "word_adjph_pos = word_phrase(adjph_pos)\r\n",
    "print('First 10 adjective phrases in positive sentences: ', word_adjph_pos[:10])\r\n",
    "\r\n",
    "# RANKING POSITIVE PHRASES BY FREQUENCY\r\n",
    "most_common_adjph_pos = get_frequency(word_adjph_pos)\r\n",
    "print(\"Top 50 adjective phrases in positive sentences: \", most_common_adjph_pos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Adjective phrases in positive sentences, with POS:  [Tree('ADJPH', [('usually', 'RB'), ('plagiaristic', 'JJ')]), Tree('ADJPH', [('more', 'RBR'), ('riotous', 'JJ')]), Tree('ADJPH', [('not', 'RB'), ('likely', 'JJ')])]\n",
      "First 10 adjective phrases in positive sentences:  ['usually plagiaristic ', 'more riotous ', 'not likely ', 'rather hard-boiled ', 'very grave ', 'so much ', 'so much ', 'rather literary ', 'very solemn ', 'not perfect ']\n",
      "Top 50 adjective phrases in positive sentences:  [('so much ', 3), ('very much ', 2), ('usually plagiaristic ', 1), ('more riotous ', 1), ('not likely ', 1), ('rather hard-boiled ', 1), ('very grave ', 1), ('rather literary ', 1), ('very solemn ', 1), ('not perfect ', 1), ('less fashionable ', 1), ('most superficial ', 1), ('only fifty ', 1), ('most powerful ', 1), ('there unrestfully wherever ', 1), ('even more ', 1), ('m stronger ', 1), ('never intimate ', 1), ('completely stationary ', 1), ('quite likely ', 1), ('very witty ', 1), ('back again—the ', 1), ('anywhere else. ', 1), ('as long ', 1), ('sharply different ', 1), ('continually disappointed ', 1), ('over tonight. ', 1), ('very romantic ', 1), ('perfectly tangible ', 1), ('pleasantly interested ', 1), ('m sophisticated ', 1), ('very next ', 1), ('m too poor. ', 1), ('” insisted ', 1), ('little disgusted ', 1), ('really less ', 1), ('not alone—fifty ', 1), ('once more ', 1), ('pretty slow ', 1), ('immediately perceptible ', 1), ('s so dumb ', 1), ('almost pastoral ', 1), ('entirely too large ', 1), ('more rakish ', 1), ('most respectful ', 1), ('up here last ', 1), ('felt just as good ', 1), ('Yet high ', 1), ('so excited ', 1), ('enough coloured ', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Great Gatsby Nouns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "def grammar_phrases(tags_sent):\r\n",
    "    grammar_adjph = \"NNPH: {<JJ.?>+<NN.?>}\" # REMEMBER TO EDI THIS GRAMMAR FOR ADVERBS & VERBS!\r\n",
    "    chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\r\n",
    "    adjph_tags = []\r\n",
    "    for sent in tags_sent:\r\n",
    "        if len(sent) > 0:\r\n",
    "            tree = chunk_parser_adj.parse(sent)\r\n",
    "            for subtree in tree.subtrees():\r\n",
    "                if subtree.label() == 'NNPH': # THIS ALSO NEEDS EDITION FOR ADVERBS & VERBS\r\n",
    "                    adjph_tags.append(subtree)\r\n",
    "    return adjph_tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "# EXTRACTING POSITIVE PHRASES AND THEIR POS\r\n",
    "adjph_pos = grammar_phrases(tags_pos)\r\n",
    "print('noun phrases in positive sentences, with POS: ', adjph_pos[:3])\r\n",
    "\r\n",
    "# EXTRACTING POSITIVE ADJECTIVE PHRASES (WORDS ONLY)\r\n",
    "word_adjph_pos = word_phrase(adjph_pos)\r\n",
    "print('First 10 noun phrases in positive sentences: ', word_adjph_pos[:10])\r\n",
    "\r\n",
    "# RANKING POSITIVE PHRASES BY FREQUENCY\r\n",
    "most_common_adjph_pos = get_frequency(word_adjph_pos)\r\n",
    "print(\"Top 50 Noun phrases in positive sentences: \", most_common_adjph_pos)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "noun phrases in positive sentences, with POS:  [Tree('NNPH', [('infinite', 'JJ'), ('hope', 'NN')]), Tree('NNPH', [('warm', 'JJ'), ('centre', 'NN')]), Tree('NNPH', [('ragged', 'JJ'), ('edge', 'NN')])]\n",
      "First 10 noun phrases in positive sentences:  ['infinite hope ', 'warm centre ', 'ragged edge ', 'various delays ', 'great bursts ', 'fast movies ', 'familiar conviction ', 'high intention ', 'many other books ', 'enormous eggs ']\n",
      "Top 50 Noun phrases in positive sentences:  [('’ t ', 36), ('” “ ', 15), ('“ Come ', 5), ('“ Look ', 4), ('Good night ', 3), ('old sport ', 3), ('“ Mr ', 3), ('” Tom ', 3), ('s voice ', 3), ('… Hot ', 3), ('me. ” ', 2), ('“ Yes ', 2), ('true. ” ', 2), ('“ Mrs ', 2), ('it. ” ', 2), ('“ See ', 2), ('yourself. ” ', 2), ('interested way ', 2), ('green light ', 2), ('well. ” ', 2), ('small investigation ', 2), ('re crazy ', 2), ('“ Jimmy ', 2), ('infinite hope ', 1), ('warm centre ', 1), ('ragged edge ', 1), ('various delays ', 1), ('great bursts ', 1), ('fast movies ', 1), ('familiar conviction ', 1), ('high intention ', 1), ('many other books ', 1), ('enormous eggs ', 1), ('domesticated body ', 1), ('salt water ', 1), ('great wet barnyard ', 1), ('s mansion ', 1), ('sturdy straw-haired man ', 1), ('hard mouth ', 1), ('supercilious manner ', 1), ('arrogant eyes ', 1), ('enormous leverage—a ', 1), ('few minutes ', 1), ('sunny porch ', 1), ('nice place ', 1), ('inside. ” ', 1), ('high hallway ', 1), ('bright rosy-coloured space ', 1), ('French windows ', 1), ('fresh grass ', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "adjph_neg = grammar_phrases(tags_neg)\r\n",
    "print('Noun pharases in negative sentences, with POS: ', adjph_neg[:3])\r\n",
    "\r\n",
    "word_adjph_neg = word_phrase(adjph_neg)\r\n",
    "print('First 10 noun phrases in negative sentences: ', word_adjph_neg[:10])\r\n",
    "\r\n",
    "most_common_adjph_neg = get_frequency(word_adjph_neg)\r\n",
    "print('Top 50 noun phrarses in negative sentences: ', most_common_adjph_neg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Noun pharases in negative sentences, with POS:  [Tree('NNPH', [('many', 'JJ'), ('curious', 'JJ'), ('natures', 'NNS')]), Tree('NNPH', [('few', 'JJ'), ('veteran', 'NN')]), Tree('NNPH', [('abnormal', 'JJ'), ('mind', 'NN')])]\n",
      "First 10 noun phrases in negative sentences:  ['many curious natures ', 'few veteran ', 'abnormal mind ', 'normal person ', 'secret griefs ', 'unknown men ', 'hostile levity ', 'unmistakable sign ', 'intimate revelation ', 'intimate revelations ']\n",
      "Top 50 noun phrarses in negative sentences:  [('’ t ', 123), ('” “ ', 36), ('old sport ', 10), ('o ’ ', 7), ('long time ', 7), ('front door ', 7), ('old sport. ', 6), ('young man ', 5), ('few minutes ', 5), ('it. ” ', 5), ('young men ', 4), ('all. ” ', 4), ('“ Look ', 4), ('young women ', 3), ('other girl ', 3), ('now. ” ', 3), ('several times ', 3), ('marble steps ', 3), ('next door ', 3), ('“ Let ', 3), ('bad driver ', 3), ('other people ', 3), ('first time ', 3), ('“ Daisy ', 3), ('s voice ', 3), ('yellow car ', 3), ('prep school ', 2), ('last minute ', 2), ('few days ', 2), ('little sinister ', 2), ('neighbour ’ ', 2), ('warm windy ', 2), ('longest day ', 2), ('hundred people ', 2), ('good deal ', 2), ('down. ” ', 2), ('several people ', 2), ('first names ', 2), ('back door ', 2), ('next remark ', 2), ('good time ', 2), ('“ Don ', 2), ('several weeks ', 2), ('many years ', 2), ('” “ Yes ', 2), ('ve heard ', 2), ('fifty years ', 2), ('s Series ', 2), ('“ Come ', 2), ('white car ', 2)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gatsby Verbs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "def grammar_phrases(tags_sent):\r\n",
    "    grammar_adjph = \"VBPH: {<RB.?>+<VB.?>}\" # REMEMBER TO EDI THIS GRAMMAR FOR ADVERBS & VERBS!\r\n",
    "    chunk_parser_adj = nltk.RegexpParser(grammar_adjph)\r\n",
    "    adjph_tags = []\r\n",
    "    for sent in tags_sent:\r\n",
    "        if len(sent) > 0:\r\n",
    "            tree = chunk_parser_adj.parse(sent)\r\n",
    "            for subtree in tree.subtrees():\r\n",
    "                if subtree.label() == 'VBPH': # THIS ALSO NEEDS EDITION FOR ADVERBS & VERBS\r\n",
    "                    adjph_tags.append(subtree)\r\n",
    "    return adjph_tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "# EXTRACTING POSITIVE PHRASES AND THEIR POS\r\n",
    "adjph_pos = grammar_phrases(tags_pos)\r\n",
    "print('Verb phrases in positive sentences, with POS: ', adjph_pos[:3])\r\n",
    "\r\n",
    "# EXTRACTING POSITIVE ADJECTIVE PHRASES (WORDS ONLY)\r\n",
    "word_adjph_pos = word_phrase(adjph_pos)\r\n",
    "print('First 10 verb phrases in positive sentences: ', word_adjph_pos[:10])\r\n",
    "\r\n",
    "# RANKING POSITIVE PHRASES BY FREQUENCY\r\n",
    "most_common_adjph_pos = get_frequency(word_adjph_pos)\r\n",
    "print(\"Top 50 verb phrases in positive sentences: \", most_common_adjph_pos)#"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Verb phrases in positive sentences, with POS:  [Tree('VBPH', [('now', 'RB'), ('seemed', 'VBZ')]), Tree('VBPH', [('casually', 'RB'), ('conferred', 'VBN')]), Tree('VBPH', [('always', 'RB'), ('leaning', 'VBG')])]\n",
      "First 10 verb phrases in positive sentences:  ['now seemed ', 'casually conferred ', 'always leaning ', 'fragilely bound ', 'ever seen ', 'ever get ', 'll be ', 's been ', 'not mentioned ', 've gotten ']\n",
      "Top 50 verb phrases in positive sentences:  [('never loved ', 6), ('ve been ', 5), ('ll be ', 4), ('ever seen ', 2), ('s been ', 2), ('d seen ', 2), ('just shows ', 2), ('now seemed ', 1), ('casually conferred ', 1), ('always leaning ', 1), ('fragilely bound ', 1), ('ever get ', 1), ('not mentioned ', 1), ('ve gotten ', 1), ('apparently finding ', 1), ('s going ', 1), ('no longer nourished ', 1), ('already crumbling ', 1), ('just meant— ', 1), ('around looking ', 1), ('just slip ', 1), ('only get ', 1), ('just got ', 1), ('almost made ', 1), ('actually invited ', 1), ('never care ', 1), ('always have ', 1), ('somewhat drunk ', 1), ('Absolutely real—have ', 1), ('somewhere before. ', 1), ('then concentrated ', 1), ('attractively tight ', 1), ('suddenly standing ', 1), ('not confined ', 1), ('slightly raised ', 1), ('just heard ', 1), ('suddenly there seemed ', 1), ('deliberately shifted ', 1), ('always gathered ', 1), ('continually breaking ', 1), ('then came ', 1), ('Then came ', 1), ('ever set ', 1), ('then added ', 1), ('effectually prevented ', 1), ('now assumed ', 1), ('once ceased ', 1), ('nearly toppled ', 1), ('now vanished ', 1), ('almost touching ', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "adjph_neg = grammar_phrases(tags_neg)\r\n",
    "print('Verb pharases in negative sentences, with POS: ', adjph_neg[:3])\r\n",
    "\r\n",
    "word_adjph_neg = word_phrase(adjph_neg)\r\n",
    "print('First 10 verb phrases in negative sentences: ', word_adjph_neg[:10])\r\n",
    "\r\n",
    "most_common_adjph_neg = get_frequency(word_adjph_neg)\r\n",
    "print('Top 50 verb phrarses in negative sentences: ', most_common_adjph_neg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Verb pharases in negative sentences, with POS:  [Tree('VBPH', [('m', 'RB'), ('inclined', 'VBN')]), Tree('VBPH', [('also', 'RB'), ('made', 'VBD')]), Tree('VBPH', [('unjustly', 'RB'), ('accused', 'VBN')])]\n",
      "First 10 verb phrases in negative sentences:  ['m inclined ', 'also made ', 'unjustly accused ', 'snobbishly suggested ', 'never found ', 'ever find ', 'temporarily closed ', 're descended ', 'never saw ', 'finally said ']\n",
      "Top 50 verb phrarses in negative sentences:  [('ve been ', 8), ('m going ', 8), ('ll be ', 5), ('there was ', 5), ('d been ', 5), ('d be ', 2), ('always watch ', 2), ('then miss ', 2), ('about was ', 2), ('ll do ', 2), ('re going ', 2), ('ever knew ', 2), ('never seen ', 2), ('d never seen ', 2), ('not been ', 2), ('t be ', 2), ('always been ', 2), ('hardly knew ', 2), ('then turned ', 2), ('never loved ', 2), ('just got ', 2), ('s going ', 2), ('never seemed ', 2), ('m inclined ', 1), ('also made ', 1), ('unjustly accused ', 1), ('snobbishly suggested ', 1), ('never found ', 1), ('ever find ', 1), ('temporarily closed ', 1), ('re descended ', 1), ('never saw ', 1), ('finally said ', 1), ('just left ', 1), ('more recently arrived ', 1), ('much more successfully looked ', 1), ('really begins ', 1), ('ever played ', 1), ('afterward savours ', 1), ('enormously wealthy—even ', 1), ('rather took ', 1), ('then drifted ', 1), ('scarcely knew ', 1), ('always had ', 1), ('then rippled ', 1), ('just been ', 1), ('almost surprised ', 1), ('so much wanted ', 1), ('ve heard ', 1), ('less charming ', 1)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "for i in range(0, len(pos_sent)):\r\n",
    "    group = {'Text':pos_sent[i], \"Sentiment\":\"Positive\", \"Book\":\"The Great Gatsby\", \"Author\":\"F Scott FitzGerald\", \"Total Postive Sentences\": pos_count, 'Total Negative Sentences':neg_count, 'Total Neutral Sentences':neu_count}\r\n",
    "    tagged_data.append(group)\r\n",
    "for i in range(0, len(neg_sent)):\r\n",
    "    group = {'Text':neg_sent[i], 'Sentiment':'Negative', \"Book\":\"The Great Gatsby\", \"Author\":\"F Scott FitzGerald\", \"Total Postive Sentences\": pos_count, 'Total Negative Sentences':neg_count, 'Total Neutral Sentences':neu_count}\r\n",
    "    tagged_data.append(group)\r\n",
    "for i in range(0, len(neu_sent)):\r\n",
    "    group = {'Text':neu_sent[i], 'Sentiment':'Neutral', \"Book\":\"The Great Gatsby\", \"Author\":\"F Scott FitzGerald\", \"Total Postive Sentences\": pos_count, 'Total Negative Sentences':neg_count, 'Total Neutral Sentences':neu_count}\r\n",
    "    tagged_data.append(group)\r\n",
    "\r\n",
    "df = pd.DataFrame(tagged_data)\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Book</th>\n",
       "      <th>Author</th>\n",
       "      <th>Total Postive Sentences</th>\n",
       "      <th>Total Negative Sentences</th>\n",
       "      <th>Total Neutral Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[, Illustration, ], ANNA, KARENINA, by, Leo, ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Anna Karenina</td>\n",
       "      <td>Leo Tolstoy</td>\n",
       "      <td>5635</td>\n",
       "      <td>6836</td>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Everything, was, in, confusion, in, the, Oblo...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Anna Karenina</td>\n",
       "      <td>Leo Tolstoy</td>\n",
       "      <td>5635</td>\n",
       "      <td>6836</td>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[“, Now, ,, how, was, it, ?]</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Anna Karenina</td>\n",
       "      <td>Leo Tolstoy</td>\n",
       "      <td>5635</td>\n",
       "      <td>6836</td>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Yes, ,, but, then, ,, Darmstadt, was, in, Ame...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Anna Karenina</td>\n",
       "      <td>Leo Tolstoy</td>\n",
       "      <td>5635</td>\n",
       "      <td>6836</td>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Stepan, Arkadyevitch, ’, s, eyes, twinkled, g...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Anna Karenina</td>\n",
       "      <td>Leo Tolstoy</td>\n",
       "      <td>5635</td>\n",
       "      <td>6836</td>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment           Book  \\\n",
       "0  [[, Illustration, ], ANNA, KARENINA, by, Leo, ...  Positive  Anna Karenina   \n",
       "1  [Everything, was, in, confusion, in, the, Oblo...  Positive  Anna Karenina   \n",
       "2                       [“, Now, ,, how, was, it, ?]  Positive  Anna Karenina   \n",
       "3  [Yes, ,, but, then, ,, Darmstadt, was, in, Ame...  Positive  Anna Karenina   \n",
       "4  [Stepan, Arkadyevitch, ’, s, eyes, twinkled, g...  Positive  Anna Karenina   \n",
       "\n",
       "        Author  Total Postive Sentences  Total Negative Sentences  \\\n",
       "0  Leo Tolstoy                     5635                      6836   \n",
       "1  Leo Tolstoy                     5635                      6836   \n",
       "2  Leo Tolstoy                     5635                      6836   \n",
       "3  Leo Tolstoy                     5635                      6836   \n",
       "4  Leo Tolstoy                     5635                      6836   \n",
       "\n",
       "   Total Neutral Sentences  \n",
       "0                     4433  \n",
       "1                     4433  \n",
       "2                     4433  \n",
       "3                     4433  \n",
       "4                     4433  "
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "# Ensure that that dataframe contains both books\r\n",
    "df.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Book</th>\n",
       "      <th>Author</th>\n",
       "      <th>Total Postive Sentences</th>\n",
       "      <th>Total Negative Sentences</th>\n",
       "      <th>Total Neutral Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19336</th>\n",
       "      <td>[By, God, it, was, awful—, ”, I, couldn, ’, t,...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F Scott FitzGerald</td>\n",
       "      <td>817</td>\n",
       "      <td>948</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19337</th>\n",
       "      <td>[One, night, I, did, hear, a, material, car, t...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F Scott FitzGerald</td>\n",
       "      <td>817</td>\n",
       "      <td>948</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19338</th>\n",
       "      <td>[Then, I, wandered, down, to, the, beach, and,...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F Scott FitzGerald</td>\n",
       "      <td>817</td>\n",
       "      <td>948</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19339</th>\n",
       "      <td>[And, as, the, moon, rose, higher, the, inesse...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F Scott FitzGerald</td>\n",
       "      <td>817</td>\n",
       "      <td>948</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19340</th>\n",
       "      <td>[And, as, I, sat, there, brooding, on, the, ol...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F Scott FitzGerald</td>\n",
       "      <td>817</td>\n",
       "      <td>948</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Sentiment  \\\n",
       "19336  [By, God, it, was, awful—, ”, I, couldn, ’, t,...   Neutral   \n",
       "19337  [One, night, I, did, hear, a, material, car, t...   Neutral   \n",
       "19338  [Then, I, wandered, down, to, the, beach, and,...   Neutral   \n",
       "19339  [And, as, the, moon, rose, higher, the, inesse...   Neutral   \n",
       "19340  [And, as, I, sat, there, brooding, on, the, ol...   Neutral   \n",
       "\n",
       "                   Book              Author  Total Postive Sentences  \\\n",
       "19336  The Great Gatsby  F Scott FitzGerald                      817   \n",
       "19337  The Great Gatsby  F Scott FitzGerald                      817   \n",
       "19338  The Great Gatsby  F Scott FitzGerald                      817   \n",
       "19339  The Great Gatsby  F Scott FitzGerald                      817   \n",
       "19340  The Great Gatsby  F Scott FitzGerald                      817   \n",
       "\n",
       "       Total Negative Sentences  Total Neutral Sentences  \n",
       "19336                       948                      672  \n",
       "19337                       948                      672  \n",
       "19338                       948                      672  \n",
       "19339                       948                      672  \n",
       "19340                       948                      672  "
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "df.to_csv('Homework3_SentimateAnalysis.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "f15837fdae91b30dab61da71d8b87089474f4d12bf60a1f1e36f7e67523581b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}